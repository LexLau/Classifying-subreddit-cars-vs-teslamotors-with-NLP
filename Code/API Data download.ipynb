{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime as dt\n",
    "import time\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from string import ascii_uppercase\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping/Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function below to engage with the reddit website and pull the content based on subreddit topics that we will feed in. We are requesting content from the last 150 days to gather a large enough dataset to generate significant results. We are also delaying each iteration by 2 seconds to ensure we do not overload the server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're asking pushshift to engage with redit website\n",
    "def query_pushshift(subreddit, kind='submission', skip=30, times=5, # we are pulling 30 days of posts 5 times = 150 days\n",
    "                    subfield = ['title', 'selftext', 'subreddit', 'created_utc', 'author', 'num_comments', \n",
    "                                'score', 'is_self'], #\n",
    "                    comfields = ['body', 'score', 'created_utc']):\n",
    "    stem = \"https://api.pushshift.io/reddit/search/{}/?subreddit={}&size=500\".format(kind, subreddit)\n",
    "    \n",
    "    mylist = []\n",
    "    \n",
    "    for x in range(1, times + 1):\n",
    "        \n",
    "        URL = \"{}&after={}d\".format(stem, skip * x)\n",
    "        print(URL)\n",
    "        response = requests.get(URL)\n",
    "        assert response.status_code == 200\n",
    "        mine = response.json()['data']\n",
    "        df = pd.DataFrame.from_dict(mine)\n",
    "        mylist.append(df)\n",
    "        time.sleep(2) # We will wait 2 seconds each iteration to prevent overloading the server\n",
    "        \n",
    "    full = pd.concat(mylist, sort=False)\n",
    "    \n",
    "    if kind == \"submission\":\n",
    "        \n",
    "        full = full[subfield]\n",
    "        \n",
    "        full = full.drop_duplicates()\n",
    "        \n",
    "        full = full.loc[full['is_self'] == True]\n",
    "        \n",
    "    def get_date(created):\n",
    "        return dt.date.fromtimestamp(created)\n",
    "    \n",
    "    _timestamp = full[\"created_utc\"].apply(get_date)\n",
    "    \n",
    "    full['timestamp'] = _timestamp\n",
    "    print(full.shape)\n",
    "    \n",
    "    return full "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running our query push shift functions on our 2 reddit topics below, directly related to our problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=30d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=60d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=90d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=120d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=150d\n",
      "(1368, 9)\n"
     ]
    }
   ],
   "source": [
    "sub_1_query = query_pushshift('teslamotors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=30d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=60d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=90d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=120d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=150d\n",
      "(1671, 9)\n"
     ]
    }
   ],
   "source": [
    "sub_2_query = query_pushshift('cars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are combining the content from both subreddits into a single dataframe below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sub_queries = pd.concat([sub_1_query, sub_2_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sub_queries.to_csv('../Data/subreddit_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
