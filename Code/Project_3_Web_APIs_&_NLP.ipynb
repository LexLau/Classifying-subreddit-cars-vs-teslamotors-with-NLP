{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Alex Lau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Table of Contents](#1.-Table-of-Contents)\n",
    "2. [Loading Libraries](#2.-Loading-Libraries)\n",
    "3. [Load in Data](#3.-Load-in-Data)\n",
    "4. [Exploratory Data Analysis (EDA)](#4.-Exploratory-Data-Analysis-(EDA))\n",
    "5. [Data Cleaning](#5.-Data-Cleaning)\n",
    "6. [Model Preperation (Preprocessing)](#6.-Model-Preparation-(Preprocessing))\n",
    "7. [Modeling](#7.-Modeling)\n",
    "    <br>7.1 [Baseline Model](#7.1-Baseline-Model)\n",
    "    <br>7.2 [Count Vectorized Logistic Regression](#7.2-Count-Vectorized-Logistic-Regression)\n",
    "    <br>7.3 [TF-IDF Vectorized Logistic Regression](#7.3-TFIDF-Vectorized-Logistic-Regression)\n",
    "    <br>7.4 [Multinomial Naive Bayes Classifier](#7.4-Multinomial-Naive-Bayes-Classifier)\n",
    "    <br>7.5 [Gaussian Naive Bayes Classifier](#7.5-Gaussian-Naive-Bayes-Classifier)\n",
    "8. [Model Selection](#8.-Model-Selection)\n",
    "9. [Model Evaluation](#9.-Model-Evaluation)\n",
    "10. [Conclusions and Evaluation](#10.-Conclusions-and-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime as dt\n",
    "import time\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from string import ascii_uppercase\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a separate notebook, we created a function to engage with the reddit website and pull the content based on subreddit topics that we fed in. We have requested content from the last 150 days to gather a large enough dataset to generate significant results. We also delayed each iteration by 2 seconds to ensure we did not overload the server. Finally, we combined API pulls for both subreddits 'cars' and 'teslamotors' into a single dataframe, which we are reading in below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sub_queries = pd.read_csv('../Data/subreddit_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3039, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the length of columns and rows\n",
    "combined_sub_queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       int64\n",
       "title           object\n",
       "selftext        object\n",
       "subreddit       object\n",
       "created_utc      int64\n",
       "author          object\n",
       "num_comments     int64\n",
       "score            int64\n",
       "is_self           bool\n",
       "timestamp       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying the data types for all columns\n",
    "combined_sub_queries.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Finally getting FSD HW3.0</td>\n",
       "      <td>I just called service center and asked for HW3...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581119843</td>\n",
       "      <td>name_nt_important</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Questions regarding destination chargers</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581120818</td>\n",
       "      <td>lukethenoteable</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Is this normal for regen braking?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581121537</td>\n",
       "      <td>Treesten</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Regen braking issue</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581121975</td>\n",
       "      <td>Treesten</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Please stop pasting your stupid ads for your T...</td>\n",
       "      <td>[I can't be the only one who is annoyed by thi...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581124918</td>\n",
       "      <td>tesrella</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0                          Finally getting FSD HW3.0   \n",
       "1           1           Questions regarding destination chargers   \n",
       "2           4                  Is this normal for regen braking?   \n",
       "3           5                                Regen braking issue   \n",
       "4           7  Please stop pasting your stupid ads for your T...   \n",
       "\n",
       "                                            selftext    subreddit  \\\n",
       "0  I just called service center and asked for HW3...  teslamotors   \n",
       "1                                          [removed]  teslamotors   \n",
       "2                                          [removed]  teslamotors   \n",
       "3                                          [removed]  teslamotors   \n",
       "4  [I can't be the only one who is annoyed by thi...  teslamotors   \n",
       "\n",
       "   created_utc             author  num_comments  score  is_self   timestamp  \n",
       "0   1581119843  name_nt_important             0      1     True  2020-02-07  \n",
       "1   1581120818    lukethenoteable             0      1     True  2020-02-07  \n",
       "2   1581121537           Treesten             0      1     True  2020-02-07  \n",
       "3   1581121975           Treesten             0      1     True  2020-02-07  \n",
       "4   1581124918           tesrella           198      1     True  2020-02-07  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the first 5 rows\n",
    "combined_sub_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>491</td>\n",
       "      <td>Abarth 595 esseesse/competizione</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571048824</td>\n",
       "      <td>MakeMemesFuckBitches</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <td>495</td>\n",
       "      <td>Weekly - What Car Should I Buy Megathread</td>\n",
       "      <td>\\n#**Weekly - What Car Should I Buy Megathread...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571051433</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>467</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>496</td>\n",
       "      <td>Identifying shady mis-badged cars/vans when tr...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571051526</td>\n",
       "      <td>KDE_Fan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>497</td>\n",
       "      <td>Car parts assistance needes</td>\n",
       "      <td>Currently I only know one website which suppli...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571052604</td>\n",
       "      <td>Papapene-bigpene</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>499</td>\n",
       "      <td>What's the point of heel toe downshift in dail...</td>\n",
       "      <td>What makes it different from: approaching corn...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571053396</td>\n",
       "      <td>Deg1935</td>\n",
       "      <td>120</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "3034         491                   Abarth 595 esseesse/competizione   \n",
       "3035         495          Weekly - What Car Should I Buy Megathread   \n",
       "3036         496  Identifying shady mis-badged cars/vans when tr...   \n",
       "3037         497                        Car parts assistance needes   \n",
       "3038         499  What's the point of heel toe downshift in dail...   \n",
       "\n",
       "                                               selftext subreddit  \\\n",
       "3034                                          [removed]      cars   \n",
       "3035  \\n#**Weekly - What Car Should I Buy Megathread...      cars   \n",
       "3036                                          [removed]      cars   \n",
       "3037  Currently I only know one website which suppli...      cars   \n",
       "3038  What makes it different from: approaching corn...      cars   \n",
       "\n",
       "      created_utc                author  num_comments  score  is_self  \\\n",
       "3034   1571048824  MakeMemesFuckBitches             2      1     True   \n",
       "3035   1571051433         AutoModerator           467     40     True   \n",
       "3036   1571051526               KDE_Fan             1      1     True   \n",
       "3037   1571052604      Papapene-bigpene             2      0     True   \n",
       "3038   1571053396               Deg1935           120     40     True   \n",
       "\n",
       "       timestamp  \n",
       "3034  2019-10-14  \n",
       "3035  2019-10-14  \n",
       "3036  2019-10-14  \n",
       "3037  2019-10-14  \n",
       "3038  2019-10-14  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the last 5 rows\n",
    "combined_sub_queries.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3039.000000</td>\n",
       "      <td>3.039000e+03</td>\n",
       "      <td>3039.000000</td>\n",
       "      <td>3039.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>252.357025</td>\n",
       "      <td>1.576260e+09</td>\n",
       "      <td>19.358671</td>\n",
       "      <td>3.706482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>144.598183</td>\n",
       "      <td>3.743054e+06</td>\n",
       "      <td>54.927408</td>\n",
       "      <td>33.728382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.570751e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>127.000000</td>\n",
       "      <td>1.573431e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.576119e+09</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>376.500000</td>\n",
       "      <td>1.578786e+09</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>1.581621e+09</td>\n",
       "      <td>741.000000</td>\n",
       "      <td>1359.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0   created_utc  num_comments        score\n",
       "count  3039.000000  3.039000e+03   3039.000000  3039.000000\n",
       "mean    252.357025  1.576260e+09     19.358671     3.706482\n",
       "std     144.598183  3.743054e+06     54.927408    33.728382\n",
       "min       0.000000  1.570751e+09      0.000000     0.000000\n",
       "25%     127.000000  1.573431e+09      0.000000     1.000000\n",
       "50%     255.000000  1.576119e+09      2.000000     1.000000\n",
       "75%     376.500000  1.578786e+09     13.000000     1.000000\n",
       "max     499.000000  1.581621e+09    741.000000  1359.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewing the statistics of the columns, only limited to 3 numeric columns\n",
    "combined_sub_queries.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing self-text values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "title            0\n",
       "selftext        44\n",
       "subreddit        0\n",
       "created_utc      0\n",
       "author           0\n",
       "num_comments     0\n",
       "score            0\n",
       "is_self          0\n",
       "timestamp        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idenfifying missing values\n",
    "combined_sub_queries.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Questions regarding destination chargers</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581120818</td>\n",
       "      <td>lukethenoteable</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Is this normal for regen braking?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581121537</td>\n",
       "      <td>Treesten</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Regen braking issue</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581121975</td>\n",
       "      <td>Treesten</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>Model 3: Any pothole popping with 19\" wheels?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581126318</td>\n",
       "      <td>teshreve</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>Does anyone know if the Model Y 20” induction ...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581130033</td>\n",
       "      <td>chas828</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>488</td>\n",
       "      <td>Need accident/insurance advice</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571046798</td>\n",
       "      <td>flexlex24</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>489</td>\n",
       "      <td>Does anyone like this style of alloy wheel/hub...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571047095</td>\n",
       "      <td>SubaruToyotaFan1986</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3033</th>\n",
       "      <td>490</td>\n",
       "      <td>Need some advice</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571047886</td>\n",
       "      <td>flexlex24</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>491</td>\n",
       "      <td>Abarth 595 esseesse/competizione</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571048824</td>\n",
       "      <td>MakeMemesFuckBitches</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>496</td>\n",
       "      <td>Identifying shady mis-badged cars/vans when tr...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571051526</td>\n",
       "      <td>KDE_Fan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1279 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "1              1           Questions regarding destination chargers   \n",
       "2              4                  Is this normal for regen braking?   \n",
       "3              5                                Regen braking issue   \n",
       "5              8      Model 3: Any pothole popping with 19\" wheels?   \n",
       "6             10  Does anyone know if the Model Y 20” induction ...   \n",
       "...          ...                                                ...   \n",
       "3031         488                     Need accident/insurance advice   \n",
       "3032         489  Does anyone like this style of alloy wheel/hub...   \n",
       "3033         490                                   Need some advice   \n",
       "3034         491                   Abarth 595 esseesse/competizione   \n",
       "3036         496  Identifying shady mis-badged cars/vans when tr...   \n",
       "\n",
       "       selftext    subreddit  created_utc                author  num_comments  \\\n",
       "1     [removed]  teslamotors   1581120818       lukethenoteable             0   \n",
       "2     [removed]  teslamotors   1581121537              Treesten             0   \n",
       "3     [removed]  teslamotors   1581121975              Treesten             0   \n",
       "5     [removed]  teslamotors   1581126318              teshreve             0   \n",
       "6     [removed]  teslamotors   1581130033               chas828             0   \n",
       "...         ...          ...          ...                   ...           ...   \n",
       "3031  [removed]         cars   1571046798             flexlex24             2   \n",
       "3032  [removed]         cars   1571047095   SubaruToyotaFan1986             1   \n",
       "3033  [removed]         cars   1571047886             flexlex24             2   \n",
       "3034  [removed]         cars   1571048824  MakeMemesFuckBitches             2   \n",
       "3036  [removed]         cars   1571051526               KDE_Fan             1   \n",
       "\n",
       "      score  is_self   timestamp  \n",
       "1         1     True  2020-02-07  \n",
       "2         1     True  2020-02-07  \n",
       "3         1     True  2020-02-07  \n",
       "5         1     True  2020-02-07  \n",
       "6         1     True  2020-02-07  \n",
       "...     ...      ...         ...  \n",
       "3031      1     True  2019-10-14  \n",
       "3032      1     True  2019-10-14  \n",
       "3033      1     True  2019-10-14  \n",
       "3034      1     True  2019-10-14  \n",
       "3036      1     True  2019-10-14  \n",
       "\n",
       "[1279 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the posts without self-texts\n",
    "combined_sub_queries[combined_sub_queries['selftext']=='[removed]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1310 or over 44% of posts have had their self-texts removed and a small portion do not have any values populated in this column. We will be focusing on the titles columns instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Finally getting FSD HW3.0</td>\n",
       "      <td>I just called service center and asked for HW3...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581119843</td>\n",
       "      <td>name_nt_important</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Questions regarding destination chargers</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581120818</td>\n",
       "      <td>lukethenoteable</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Is this normal for regen braking?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581121537</td>\n",
       "      <td>Treesten</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Regen braking issue</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581121975</td>\n",
       "      <td>Treesten</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Please stop pasting your stupid ads for your T...</td>\n",
       "      <td>[I can't be the only one who is annoyed by thi...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581124918</td>\n",
       "      <td>tesrella</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>Model 3: Any pothole popping with 19\" wheels?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581126318</td>\n",
       "      <td>teshreve</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>Does anyone know if the Model Y 20” induction ...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581130033</td>\n",
       "      <td>chas828</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>Hello r/teslamotors new owner with some questi...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581132539</td>\n",
       "      <td>Tumpias</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>I have gone from tesla being my dream car to z...</td>\n",
       "      <td>https://www.theverge.com/platform/amp/2020/2/6...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581134437</td>\n",
       "      <td>Throwaway777mmm</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>Quick question:how do you guys work on your cars?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581136526</td>\n",
       "      <td>steve_buchemi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17</td>\n",
       "      <td>2012 Model S Lemon/Buyback</td>\n",
       "      <td>I'm currently looking at a used 2012 Model S w...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581139071</td>\n",
       "      <td>RedAngellion</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19</td>\n",
       "      <td>Test drove a Model 3 today w/o a dealer rep. O...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581140709</td>\n",
       "      <td>Datsunoffroad</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24</td>\n",
       "      <td>Daily Discussion + Support Thread - February 08</td>\n",
       "      <td>Use this recurring thread for basic Q&amp;amp;A, v...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581152962</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>I would like to buy a Tesla house</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581161050</td>\n",
       "      <td>eeddeedde</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>33</td>\n",
       "      <td>Rant about Tesla’s infotainment &amp;amp; software...</td>\n",
       "      <td>This is not about AP or FSD.\\n\\r\\nFor referenc...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581168588</td>\n",
       "      <td>allegory_corey</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34</td>\n",
       "      <td>The world needs a Tesla Van</td>\n",
       "      <td>High top Sprinter style vans are in huge deman...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581168906</td>\n",
       "      <td>Protobott</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>38</td>\n",
       "      <td>Select driver from App</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581171986</td>\n",
       "      <td>blockisland33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>39</td>\n",
       "      <td>Select Driver in the App</td>\n",
       "      <td>This is pretty self explanatory, but does anyo...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581172643</td>\n",
       "      <td>blockisland33</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>40</td>\n",
       "      <td>They should make a 2 seat tesla again</td>\n",
       "      <td>Like the ones from 2008. The very first ones.</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581172898</td>\n",
       "      <td>im2gr84u</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>42</td>\n",
       "      <td>It's worrying that the Mods deleted the Verge ...</td>\n",
       "      <td>I come here for news and information about Tes...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581177310</td>\n",
       "      <td>Kurso</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                              title  \\\n",
       "0            0                          Finally getting FSD HW3.0   \n",
       "1            1           Questions regarding destination chargers   \n",
       "2            4                  Is this normal for regen braking?   \n",
       "3            5                                Regen braking issue   \n",
       "4            7  Please stop pasting your stupid ads for your T...   \n",
       "5            8      Model 3: Any pothole popping with 19\" wheels?   \n",
       "6           10  Does anyone know if the Model Y 20” induction ...   \n",
       "7           12  Hello r/teslamotors new owner with some questi...   \n",
       "8           14  I have gone from tesla being my dream car to z...   \n",
       "9           15  Quick question:how do you guys work on your cars?   \n",
       "10          17                         2012 Model S Lemon/Buyback   \n",
       "11          19  Test drove a Model 3 today w/o a dealer rep. O...   \n",
       "12          24    Daily Discussion + Support Thread - February 08   \n",
       "13          25                  I would like to buy a Tesla house   \n",
       "14          33  Rant about Tesla’s infotainment &amp; software...   \n",
       "15          34                        The world needs a Tesla Van   \n",
       "16          38                             Select driver from App   \n",
       "17          39                           Select Driver in the App   \n",
       "18          40              They should make a 2 seat tesla again   \n",
       "19          42  It's worrying that the Mods deleted the Verge ...   \n",
       "\n",
       "                                             selftext    subreddit  \\\n",
       "0   I just called service center and asked for HW3...  teslamotors   \n",
       "1                                           [removed]  teslamotors   \n",
       "2                                           [removed]  teslamotors   \n",
       "3                                           [removed]  teslamotors   \n",
       "4   [I can't be the only one who is annoyed by thi...  teslamotors   \n",
       "5                                           [removed]  teslamotors   \n",
       "6                                           [removed]  teslamotors   \n",
       "7                                           [removed]  teslamotors   \n",
       "8   https://www.theverge.com/platform/amp/2020/2/6...  teslamotors   \n",
       "9                                           [removed]  teslamotors   \n",
       "10  I'm currently looking at a used 2012 Model S w...  teslamotors   \n",
       "11                                          [removed]  teslamotors   \n",
       "12  Use this recurring thread for basic Q&amp;A, v...  teslamotors   \n",
       "13                                          [removed]  teslamotors   \n",
       "14  This is not about AP or FSD.\\n\\r\\nFor referenc...  teslamotors   \n",
       "15  High top Sprinter style vans are in huge deman...  teslamotors   \n",
       "16                                          [removed]  teslamotors   \n",
       "17  This is pretty self explanatory, but does anyo...  teslamotors   \n",
       "18      Like the ones from 2008. The very first ones.  teslamotors   \n",
       "19  I come here for news and information about Tes...  teslamotors   \n",
       "\n",
       "    created_utc             author  num_comments  score  is_self   timestamp  \n",
       "0    1581119843  name_nt_important             0      1     True  2020-02-07  \n",
       "1    1581120818    lukethenoteable             0      1     True  2020-02-07  \n",
       "2    1581121537           Treesten             0      1     True  2020-02-07  \n",
       "3    1581121975           Treesten             0      1     True  2020-02-07  \n",
       "4    1581124918           tesrella           198      1     True  2020-02-07  \n",
       "5    1581126318           teshreve             0      1     True  2020-02-07  \n",
       "6    1581130033            chas828             0      1     True  2020-02-07  \n",
       "7    1581132539            Tumpias             0      1     True  2020-02-07  \n",
       "8    1581134437    Throwaway777mmm            27      1     True  2020-02-07  \n",
       "9    1581136526      steve_buchemi             0      1     True  2020-02-07  \n",
       "10   1581139071       RedAngellion            11      1     True  2020-02-08  \n",
       "11   1581140709      Datsunoffroad             0      1     True  2020-02-08  \n",
       "12   1581152962      AutoModerator           168      1     True  2020-02-08  \n",
       "13   1581161050          eeddeedde             0      1     True  2020-02-08  \n",
       "14   1581168588     allegory_corey           153      1     True  2020-02-08  \n",
       "15   1581168906          Protobott            37      1     True  2020-02-08  \n",
       "16   1581171986      blockisland33             0      1     True  2020-02-08  \n",
       "17   1581172643      blockisland33             4      1     True  2020-02-08  \n",
       "18   1581172898           im2gr84u             0      1     True  2020-02-08  \n",
       "19   1581177310              Kurso            49      1     True  2020-02-08  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing documents where sefttext was removed.\n",
    "combined_sub_queries.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3039 entries, 0 to 3038\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0    3039 non-null   int64 \n",
      " 1   title         3039 non-null   object\n",
      " 2   selftext      2995 non-null   object\n",
      " 3   subreddit     3039 non-null   object\n",
      " 4   created_utc   3039 non-null   int64 \n",
      " 5   author        3039 non-null   object\n",
      " 6   num_comments  3039 non-null   int64 \n",
      " 7   score         3039 non-null   int64 \n",
      " 8   is_self       3039 non-null   bool  \n",
      " 9   timestamp     3039 non-null   object\n",
      "dtypes: bool(1), int64(4), object(5)\n",
      "memory usage: 216.8+ KB\n"
     ]
    }
   ],
   "source": [
    "combined_sub_queries.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems people are slightly more engaging on subreddit 'teslamotors' given the higher mean number of comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cars</th>\n",
       "      <td>250.111909</td>\n",
       "      <td>1.576163e+09</td>\n",
       "      <td>17.910832</td>\n",
       "      <td>4.131059</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teslamotors</th>\n",
       "      <td>255.099415</td>\n",
       "      <td>1.576379e+09</td>\n",
       "      <td>21.127193</td>\n",
       "      <td>3.187865</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Unnamed: 0   created_utc  num_comments     score  is_self\n",
       "subreddit                                                             \n",
       "cars         250.111909  1.576163e+09     17.910832  4.131059     True\n",
       "teslamotors  255.099415  1.576379e+09     21.127193  3.187865     True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifying the mean values for each subreddit\n",
    "combined_sub_queries.groupby('subreddit').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for displaying bar graph of frequent terms based on the subreddit\n",
    "def frequent_terms(subreddit, ngrams):\n",
    "    cv = CountVectorizer(stop_words = 'english', ngram_range=ngrams)\n",
    "    cv.fit(combined_sub_queries.loc[combined_sub_queries['subreddit'] == subreddit,'title'])\n",
    "    X_cv = cv.transform(combined_sub_queries.loc[combined_sub_queries['subreddit'] == subreddit,'title'])\n",
    "    X_cv_df = pd.DataFrame(X_cv.todense(), columns=cv.get_feature_names())\n",
    "    bar_graph = X_cv_df.sum().sort_values(ascending = False).head(20).plot(kind = 'barh')\n",
    "    plt.title(f'Most Frequent Terms in \"{subreddit}\"', size = 16)\n",
    "    plt.xlabel('Frequency', size = 15)\n",
    "    plt.ylabel('Words', size = 15)\n",
    "    return bar_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 titles from subreddit \"cars\" unsurprisingly contains the word \"car\" many times. We can also derive that the most popular car of discussion is Honda Civic, followed by Ford Mustang, Toyota Camry, Land Rover Range Rover, and Toyota Corolla. We can also see more interest in new cars as opposed to used cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequent_terms('cars', (2,2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular term in the \"teslamotors\" subreddit, \"feature request\", refers to over-the-air (OTA) software updates that Tesla sends out over time. These are mostly wish lists, and sometimes do come true when Tesla reviews feedback. Owners will frequently tweet Elon Musk, and he will respond! The 3rd most populate bigram is \"smart summon\" which refers to the feature of using phones to control the car to drive forward, backward, or even navigate to where you are in a parking lot. \n",
    "<br>We would expect low occurances of these terms in the 'cars' subreddit since most other auto manufacturers are not as software focused, and are lagging behind in this area. \n",
    "<br>We also see frequent occurances of the term \"daily discussion\" and \"support thread\" which we will need to take out since that is a general reddit term and not subreddit specific. \n",
    "<br>Given the high level of Tesla specific terms, we are confident in our ability to accuratly match titles to their respective subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequent_terms('teslamotors', (2,2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "car         560\n",
       "tesla       368\n",
       "model       308\n",
       "cars        191\n",
       "help        109\n",
       "question    104\n",
       "new         101\n",
       "does         88\n",
       "need         84\n",
       "buy          78\n",
       "vs           73\n",
       "used         71\n",
       "feature      65\n",
       "2019         63\n",
       "like         62\n",
       "just         60\n",
       "driving      59\n",
       "best         56\n",
       "drive        56\n",
       "engine       52\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data up into X and y.\n",
    "X = combined_sub_queries['title']\n",
    "y = combined_sub_queries['subreddit'].map(lambda x: 1 if x == 'teslamotors' else 0)\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "cv.fit(X)\n",
    "X_cv = cv.transform(X)\n",
    "words = pd.DataFrame(X_cv.todense(), columns=cv.get_feature_names())\n",
    "\n",
    "# Let's look at the most frequently used words.\n",
    "words.sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00          1\n",
       "000        10\n",
       "000usd      1\n",
       "01          1\n",
       "02          1\n",
       "           ..\n",
       "zr1         2\n",
       "ดข          1\n",
       "บเคล        1\n",
       "อนไฟฟ       1\n",
       "าขนาดเล     1\n",
       "Length: 4657, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the number of words\n",
    "words.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the character lengths of the words.\n",
    "lengths_of_words = [len(each) for each in words.columns]\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(lengths_of_words, ax=ax, kde = False)\n",
    "plt.title('Histogram of Word Lengths', size = 20)\n",
    "plt.axvline(np.mean(lengths_of_words), 0,350, color = 'red', label = 'mean')\n",
    "plt.axvline(np.median(lengths_of_words), 0,350, color = 'green', label = 'median')\n",
    "plt.ylabel('Frequency', size = 15)\n",
    "plt.xlabel('Characters', size = 15)\n",
    "ax.set_xlim(1,20)\n",
    "ax.set_xticks(range(1,20))\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the character lengths of the titles.\n",
    "lengths_of_posts = [len(each) for each in X]\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(lengths_of_posts, kde = False, bins = 30) #kde = False\n",
    "plt.title('Histogram of Title Lengths', size = 20)\n",
    "plt.axvline(np.mean(lengths_of_posts), 0,350, color = 'red', label = 'mean')\n",
    "plt.axvline(np.median(lengths_of_posts), 0,350, color = 'green', label = 'median')\n",
    "plt.ylabel('Frequency', size = 15)\n",
    "plt.xlabel('Characters', size = 15)\n",
    "ax.set_xlim(1,300)\n",
    "ax.set_xticks(range(0, 300, 20))\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.53866403422178"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifying the mean character lenght of titles\n",
    "np.mean(lengths_of_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifying the median character lenght of titles\n",
    "np.median(lengths_of_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cars           1671\n",
       "teslamotors    1368\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the quantities of each class to confirm they are balanced\n",
    "combined_sub_queries['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Finally getting FSD HW3.0</td>\n",
       "      <td>I just called service center and asked for HW3...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581119843</td>\n",
       "      <td>name_nt_important</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Questions regarding destination chargers</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581120818</td>\n",
       "      <td>lukethenoteable</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Is this normal for regen braking?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581121537</td>\n",
       "      <td>Treesten</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Regen braking issue</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581121975</td>\n",
       "      <td>Treesten</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Please stop pasting your stupid ads for your T...</td>\n",
       "      <td>[I can't be the only one who is annoyed by thi...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1581124918</td>\n",
       "      <td>tesrella</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>491</td>\n",
       "      <td>Abarth 595 esseesse/competizione</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571048824</td>\n",
       "      <td>MakeMemesFuckBitches</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <td>495</td>\n",
       "      <td>Weekly - What Car Should I Buy Megathread</td>\n",
       "      <td>\\n#**Weekly - What Car Should I Buy Megathread...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571051433</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>467</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>496</td>\n",
       "      <td>Identifying shady mis-badged cars/vans when tr...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571051526</td>\n",
       "      <td>KDE_Fan</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>497</td>\n",
       "      <td>Car parts assistance needes</td>\n",
       "      <td>Currently I only know one website which suppli...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571052604</td>\n",
       "      <td>Papapene-bigpene</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>499</td>\n",
       "      <td>What's the point of heel toe downshift in dail...</td>\n",
       "      <td>What makes it different from: approaching corn...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1571053396</td>\n",
       "      <td>Deg1935</td>\n",
       "      <td>120</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-10-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3039 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "0              0                          Finally getting FSD HW3.0   \n",
       "1              1           Questions regarding destination chargers   \n",
       "2              4                  Is this normal for regen braking?   \n",
       "3              5                                Regen braking issue   \n",
       "4              7  Please stop pasting your stupid ads for your T...   \n",
       "...          ...                                                ...   \n",
       "3034         491                   Abarth 595 esseesse/competizione   \n",
       "3035         495          Weekly - What Car Should I Buy Megathread   \n",
       "3036         496  Identifying shady mis-badged cars/vans when tr...   \n",
       "3037         497                        Car parts assistance needes   \n",
       "3038         499  What's the point of heel toe downshift in dail...   \n",
       "\n",
       "                                               selftext    subreddit  \\\n",
       "0     I just called service center and asked for HW3...  teslamotors   \n",
       "1                                             [removed]  teslamotors   \n",
       "2                                             [removed]  teslamotors   \n",
       "3                                             [removed]  teslamotors   \n",
       "4     [I can't be the only one who is annoyed by thi...  teslamotors   \n",
       "...                                                 ...          ...   \n",
       "3034                                          [removed]         cars   \n",
       "3035  \\n#**Weekly - What Car Should I Buy Megathread...         cars   \n",
       "3036                                          [removed]         cars   \n",
       "3037  Currently I only know one website which suppli...         cars   \n",
       "3038  What makes it different from: approaching corn...         cars   \n",
       "\n",
       "      created_utc                author  num_comments  score  is_self  \\\n",
       "0      1581119843     name_nt_important             0      1     True   \n",
       "1      1581120818       lukethenoteable             0      1     True   \n",
       "2      1581121537              Treesten             0      1     True   \n",
       "3      1581121975              Treesten             0      1     True   \n",
       "4      1581124918              tesrella           198      1     True   \n",
       "...           ...                   ...           ...    ...      ...   \n",
       "3034   1571048824  MakeMemesFuckBitches             2      1     True   \n",
       "3035   1571051433         AutoModerator           467     40     True   \n",
       "3036   1571051526               KDE_Fan             1      1     True   \n",
       "3037   1571052604      Papapene-bigpene             2      0     True   \n",
       "3038   1571053396               Deg1935           120     40     True   \n",
       "\n",
       "       timestamp  \n",
       "0     2020-02-07  \n",
       "1     2020-02-07  \n",
       "2     2020-02-07  \n",
       "3     2020-02-07  \n",
       "4     2020-02-07  \n",
       "...          ...  \n",
       "3034  2019-10-14  \n",
       "3035  2019-10-14  \n",
       "3036  2019-10-14  \n",
       "3037  2019-10-14  \n",
       "3038  2019-10-14  \n",
       "\n",
       "[3039 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any potential duplicates\n",
    "combined_sub_queries = combined_sub_queries.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cars           1671\n",
       "teslamotors    1368\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing how balanced the data set is between the two classes\n",
    "combined_sub_queries['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have setup stop words to remove the 'daily discussion' and 'support thread' in our model preparation section, which will run during our modeling section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Preparation (Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our X and y variables\n",
    "X = combined_sub_queries['title']\n",
    "y = combined_sub_queries['subreddit']\n",
    "\n",
    "# train test splitting our data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up our classes to use later in our pipelines\n",
    "# following code found on https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    \n",
    "class StemTokenizer:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc)]\n",
    "\n",
    "# https://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required/51127115#51127115\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up custom stop words\n",
    "sw = ['daily discussion', 'support thread']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline is about 55%. This represents the majority class of subreddit 'cars' of our binary classification problem. We need to outperform this majority baseline score before we can claim usefulness from our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5498519249753209"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = y.value_counts(normalize = True).max()\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Count Vectorized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer is a transformer that tokenizes text by lowercasing and removing punctuation from words. It does not, however, convert the words to their root forms. We have introduced Porter Stemmer and WordNet Lemmatizer as a hyper-parameter in all of our models. Stemming may result in words that are not actual English words, but lemmatizing will root to actual English words. \n",
    "Logistic Regression is a linear where predictions are transformed using logistic function. There is a default penalty of L2, meaning this will automatically use the Ridge penalty and lower coefficients instead of removing them completely. We have included hyper parameters 'lbfgs', 'liblinear', and 'saga' and will have the model determine the best to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cv',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=3950, min_df=1,\n",
       "                                 ngram_range=(1, 8), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<__main__.StemTokenizer object at 0x0000020F773B2D88>,\n",
       "                                 vocabulary=None)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='saga', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate pipeline.  \n",
    "pipe_cv = Pipeline([\n",
    "    ('cv', CountVectorizer(stop_words = sw)), \n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define grid of parameters to GridSearch over.\n",
    "params_grid = {\n",
    "    'cv__tokenizer': [LemmaTokenizer(), StemTokenizer()],\n",
    "    'cv__max_features': [3950, 4000, 4050],\n",
    "    'cv__stop_words': ['english'],\n",
    "    'cv__ngram_range': [(1,7),(1,8)],\n",
    "    'lr__solver': ['lbfgs', 'liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# GridSearch over pipeline with given grid of parameters.\n",
    "gs_cv = GridSearchCV(pipe_cv, params_grid, cv=5, scoring = 'accuracy')\n",
    "\n",
    "# Fit model.\n",
    "gs_cv.fit(X_train, y_train)\n",
    "\n",
    "gs_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 3950,\n",
       " 'cv__ngram_range': (1, 8),\n",
       " 'cv__stop_words': 'english',\n",
       " 'cv__tokenizer': <__main__.StemTokenizer at 0x20f6b66e9c8>,\n",
       " 'lr__solver': 'saga'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8855"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking accuracy score\n",
    "gs_cv_lr_accuracy = np.round(gs_cv.score(X_test, y_test), 4)\n",
    "gs_cv_lr_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 TFIDF Vectorized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency inverse document frequency (TF-IDF) Vectorizer is another tranformer that offers a more sophisticated approach as it builds on Count Vectorizer by increasing weights on words that are frequent in a document or row, but offsets this by decreasing weights on words that are frequent in the corpus, or the number of documents that contain the words in question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvec',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=4050,\n",
       "                                 min_df=1, ngram_range=(1, 4), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_p...,\n",
       "                                 tokenizer=<__main__.StemTokenizer object at 0x0000020F77C1C988>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='saga', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Pipeline - TFIDF\n",
    "pipe_tfidf = Pipeline(steps = [('tfidfvec', TfidfVectorizer(stop_words = sw)),     # first tuple is for first step: vectorizer\n",
    "                         ('lr', LogisticRegression())        # second tuple is for second step: model\n",
    "                        ])    \n",
    "\n",
    "# Construct Grid Parameters\n",
    "hyperparams_grid = {\n",
    "                'tfidfvec__tokenizer': [LemmaTokenizer(), StemTokenizer()],\n",
    "                'tfidfvec__max_features': [4050, 4100, 4150],\n",
    "                'tfidfvec__ngram_range': [(1,4),(1,5),(1,6)],\n",
    "                'tfidfvec__stop_words': ['english'],\n",
    "                'lr__solver': ['liblinear', 'lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "gs_tfidf = GridSearchCV(pipe_tfidf, # pipeline object replaces what we usually had as empty model class\n",
    "                 param_grid=hyperparams_grid,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy')\n",
    "\n",
    "gs_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Get best estimator\n",
    "gs_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8868"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking accuracy score\n",
    "gs_tfidf_lr_accuracy = np.round(gs_tfidf.score(X_test, y_test),4)\n",
    "gs_tfidf_lr_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutlinomial Naive Bayes works when we have discrete data, building on Count Vectorizer using the counts of each word or token to classify and predict our y value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cvec',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=5000, min_df=1,\n",
       "                                 ngram_range=(1, 6), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<__main__.LemmaTokenizer object at 0x0000020F77EF4408>,\n",
       "                                 vocabulary=None)),\n",
       "                ('to_dense',\n",
       "                 <__main__.DenseTransformer object at 0x0000020F6B66E208>),\n",
       "                ('mnb',\n",
       "                 MultinomialNB(alpha=1, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup pipeline for Multinomial Naive Bayes\n",
    "pipe_mnb = Pipeline([\n",
    "     ('cvec', CountVectorizer(stop_words = sw)), \n",
    "     ('to_dense', DenseTransformer()), \n",
    "     ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Construct Grid Parameters\n",
    "hyperparams_grid = {\n",
    "                'cvec__tokenizer': [LemmaTokenizer(), StemTokenizer()],\n",
    "                'cvec__max_features': [5000, 5500],\n",
    "                'cvec__ngram_range': [(1,6),(1,7)],\n",
    "                'cvec__stop_words': ['english'],\n",
    "                'mnb__alpha': [0, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "gs_mnb = GridSearchCV(pipe_mnb, # pipeline object replaces what we usually had as empty model class\n",
    "                 param_grid=hyperparams_grid,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy')\n",
    "\n",
    "gs_mnb.fit(X_train, y_train)\n",
    "\n",
    "# Get best estimator\n",
    "gs_mnb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': 5000,\n",
       " 'cvec__ngram_range': (1, 6),\n",
       " 'cvec__stop_words': 'english',\n",
       " 'cvec__tokenizer': <__main__.LemmaTokenizer at 0x20f772036c8>,\n",
       " 'mnb__alpha': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8868"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking accuracy score\n",
    "gs_mnb_accuracy = np.round(gs_mnb.score(X_test, y_test), 4)\n",
    "gs_mnb_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Gaussian Naive Bayes Classifier####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes is used in cases where our features are continuous, hence we will combine this with our TF-IDF Vectorizer. A Gaussian distributions is also called a normal distribtuion, providing a bell shaped curve symmetric about the mean of the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvec',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=5500,\n",
       "                                 min_df=1, ngram_range=(1, 7), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<__main__.StemTokenizer object at 0x0000020F77E57D88>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('to_dense',\n",
       "                 <__main__.DenseTransformer object at 0x0000020F77DA60C8>),\n",
       "                ('gnb', GaussianNB(priors=None, var_smoothing=0.9))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup pipeline for Multinomial Naive Bayes\n",
    "pipe_gnb = Pipeline([\n",
    "     ('tfidfvec', TfidfVectorizer(stop_words = sw)), \n",
    "     ('to_dense', DenseTransformer()), \n",
    "     ('gnb', GaussianNB())\n",
    "])\n",
    "\n",
    "# Construct Grid Parameters\n",
    "hyperparams_grid = {\n",
    "                'tfidfvec__tokenizer': [LemmaTokenizer(), StemTokenizer()],\n",
    "                'tfidfvec__max_features': [5000, 5500, 6000],\n",
    "                'tfidfvec__ngram_range': [(1,7), (1,8)],\n",
    "                'tfidfvec__stop_words': ['english'],\n",
    "                'gnb__var_smoothing': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "gs_gnb = GridSearchCV(pipe_gnb, # pipeline object replaces what we usually had as empty model class\n",
    "                 param_grid=hyperparams_grid,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy')\n",
    "\n",
    "gs_gnb.fit(X_train, y_train)\n",
    "\n",
    "# Get best estimator\n",
    "gs_gnb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gnb__var_smoothing': 0.9,\n",
       " 'tfidfvec__max_features': 5500,\n",
       " 'tfidfvec__ngram_range': (1, 7),\n",
       " 'tfidfvec__stop_words': 'english',\n",
       " 'tfidfvec__tokenizer': <__main__.StemTokenizer at 0x20f778e3708>}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_gnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8737"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking accuracy score\n",
    "gs_gnb_accuracy = np.round(gs_gnb.score(X_test, y_test), 4)\n",
    "gs_gnb_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy score is : 0.5499.\n",
      "\n",
      "Training accuracy score for our Count Vectorized Logistic Regression model is: 0.9671.\n",
      "Testing accuracy score for our Count Vectorized Logistic Regression model is: 0.8855.\n",
      "\n",
      "Training accuracy score for our TF-IDF Vectorized Logistic Regression model is: 0.9495.\n",
      "Testing accuracy score for our TF-IDF Vectorized Logistic Regression model is: 0.8868.\n",
      "\n",
      "Training accuracy score for our Multinomial Naive Bayes Classifier model is: 0.9491.\n",
      "Testing accuracy score for our Multinomial Naive Bayes Classifier model is: 0.8868.\n",
      "\n",
      "Training accuracy score for our Gaussian Naive Bayes model is: 0.9469.\n",
      "Testing accuracy score for our Gaussian Naive Bayes model is: 0.8737.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions\n",
    "print(f'Baseline accuracy score is : {np.round(baseline, 4)}.')\n",
    "print()\n",
    "print(f'Training accuracy score for our Count Vectorized Logistic Regression model is: {round(gs_cv.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Count Vectorized Logistic Regression model is: {gs_cv_lr_accuracy}.')\n",
    "print()\n",
    "print(f'Training accuracy score for our TF-IDF Vectorized Logistic Regression model is: {round(gs_tfidf.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our TF-IDF Vectorized Logistic Regression model is: {gs_tfidf_lr_accuracy}.')\n",
    "print()\n",
    "print(f'Training accuracy score for our Multinomial Naive Bayes Classifier model is: {round(gs_mnb.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Multinomial Naive Bayes Classifier model is: {gs_mnb_accuracy}.')\n",
    "print()\n",
    "print(f'Training accuracy score for our Gaussian Naive Bayes model is: {round(gs_gnb.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Gaussian Naive Bayes model is: {gs_gnb_accuracy}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize list of lists \n",
    "scores_list = [['CountVectorized Logistic', gs_cv_lr_accuracy], ['TFIDFVectorized Logistic', gs_tfidf_lr_accuracy], \n",
    "        ['Multinomial Naive Bayes', gs_mnb_accuracy], ['Gaussian Naive Bayes', gs_gnb_accuracy]]\n",
    "  \n",
    "# Create the pandas DataFrame \n",
    "scores_df = pd.DataFrame(scores_list, columns = ['Classifier', 'Test Accuracy Score']).sort_values('Test Accuracy Score')\n",
    "# scores_df.sort_values(scores_df, ascending = False)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "ax = sns.barplot(x=\"Test Accuracy Score\", y='Classifier', data=scores_df, palette = 'Blues_d')\n",
    "plt.title(f'Test Accuracy Scores for All Models', size = 20)\n",
    "plt.xlabel('Test Accuracy Score', size = 18)\n",
    "plt.xticks(ticks = [0, 0.2, 0.4, 0.6, 0.8, 1], size = 15)\n",
    "plt.yticks(size = 15)\n",
    "plt.ylabel('Classifier', size = 18)\n",
    "\n",
    "for i in ax.patches:\n",
    "    # get_width pulls left or right; get_y pushes up or down\n",
    "    ax.text(i.get_width(), i.get_y()+.31, \\\n",
    "            str(round((i.get_width()), 4)), fontsize=15, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model in this iteration appears to be the TFIDF Vectorized Logistic Regression Model with an testing accuracy score of 88.68%, so we will select this to evaluate. This is 32 points above our baseline of 54.99%, or over 61% increase. We ran multiple iterations of our models and found that as titles of posts changed over time, our winning model would switch between TFIDF Vectorized Logistic regression and Multinomial Naive Bayes, but we never had an instance of the Gaussian Naive Bayes scoring highest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are getting our best model, the 2nd value of our grid search from TFIDF Vectorized Logistic Regression model, and our best transformer, the 1st value. This will allow us to call dot(.) attributes and methods to eventually pull our coefficients, which will help us identify the most useful words for our model to predict the subreddit 'teslamotors'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs_tfidf.best_estimator_[1]\n",
    "best_transformer = gs_tfidf.best_estimator_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = best_transformer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = best_model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    'feature':features,\n",
    "    'coef': np.round(coefs, 2),\n",
    "    'exp_coef': np.round(np.exp(coefs), 2)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exp_coef below represents the odds that a title is classified as under subreddit 'teslamotors' given the word in the feature column exists in the document. It is not surprising that given a document contains the word \"tesla\" it is 1542 times as likely to be classified under 'teslamotors'. Terms beyond the obvious make and model include supercharge: a tesla coined term for charging stations with the ability to deliver electricity at extremely fast rates, autopilot: a driver assistance safety feature with automatic emergency braking, full-self driving (fsd): an autonomous driving feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "      <th>exp_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3494</th>\n",
       "      <td>tesla</td>\n",
       "      <td>7.34</td>\n",
       "      <td>1542.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>model</td>\n",
       "      <td>4.24</td>\n",
       "      <td>69.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>model 3</td>\n",
       "      <td>3.20</td>\n",
       "      <td>24.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>3</td>\n",
       "      <td>2.68</td>\n",
       "      <td>14.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>cybertruck</td>\n",
       "      <td>2.56</td>\n",
       "      <td>12.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3414</th>\n",
       "      <td>supercharg</td>\n",
       "      <td>2.38</td>\n",
       "      <td>10.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>fsd</td>\n",
       "      <td>2.33</td>\n",
       "      <td>10.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3756</th>\n",
       "      <td>updat</td>\n",
       "      <td>2.19</td>\n",
       "      <td>8.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>charg</td>\n",
       "      <td>2.15</td>\n",
       "      <td>8.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>autopilot</td>\n",
       "      <td>2.09</td>\n",
       "      <td>8.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>featur</td>\n",
       "      <td>1.96</td>\n",
       "      <td>7.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>mode</td>\n",
       "      <td>1.73</td>\n",
       "      <td>5.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>deliveri</td>\n",
       "      <td>1.59</td>\n",
       "      <td>4.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>elon</td>\n",
       "      <td>1.56</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>m3</td>\n",
       "      <td>1.44</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3179</th>\n",
       "      <td>sentri</td>\n",
       "      <td>1.34</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2987</th>\n",
       "      <td>regen</td>\n",
       "      <td>1.29</td>\n",
       "      <td>3.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>y</td>\n",
       "      <td>1.28</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>rang</td>\n",
       "      <td>1.23</td>\n",
       "      <td>3.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>:</td>\n",
       "      <td>1.21</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature  coef  exp_coef\n",
       "3494       tesla  7.34   1542.01\n",
       "2438       model  4.24     69.36\n",
       "2439     model 3  3.20     24.53\n",
       "374            3  2.68     14.58\n",
       "1343  cybertruck  2.56     12.94\n",
       "3414  supercharg  2.38     10.80\n",
       "1828         fsd  2.33     10.24\n",
       "3756       updat  2.19      8.95\n",
       "1148       charg  2.15      8.59\n",
       "770    autopilot  2.09      8.05\n",
       "1747      featur  1.96      7.08\n",
       "2431        mode  1.73      5.66\n",
       "1425    deliveri  1.59      4.91\n",
       "1625        elon  1.56      4.75\n",
       "2320          m3  1.44      4.20\n",
       "3179      sentri  1.34      3.80\n",
       "2987       regen  1.29      3.65\n",
       "4002           y  1.28      3.60\n",
       "2924        rang  1.23      3.42\n",
       "477            :  1.21      3.35"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values(by = 'coef', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have mapped a confusion matrix of the actual subreddit vs our predictions. We were better at accurately predicting titles that belonged in the cars subreddit than teslamotors. We were 94 % accurate with our predictions from actual subreddit 'cars', but only 82% accurate with our predictions from actual subreddit 'teslamotors'. This means there are still normal car terms used in subreddit 'teslamotors' that our model was unable to differentiate between the two, but this is expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confm = confusion_matrix(y_test, y_pred)\n",
    "columns = ['cars', 'teslamotors'] # need to fix this\n",
    "df_cm = pd.DataFrame(confm, index=columns, columns=columns)\n",
    "# sns.heatmap(df_cm, cmap='Oranges', annot=True)\n",
    "plt.figure(figsize = (5, 4))\n",
    "plt.title('Confusion Matrix', size = 22)\n",
    "sns.heatmap(df_cm, annot=True, fmt = 'd', annot_kws={\"size\": 15}, cmap = 'RdBu')\n",
    "plt.ylabel('Actual', size = 20)\n",
    "plt.xlabel('Predictions', size = 20)\n",
    "plt.yticks(rotation = 0, size = 15)\n",
    "plt.xticks(size = 15)\n",
    "\n",
    "# shoutout to SalMac86 on https://github.com/mwaskom/seaborn/issues/1773\n",
    "# fix for mpl bug that cuts off top/bottom of seaborn viz\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "plt.show()\n",
    "\n",
    "## add xlabel and ylabel for predicted vs actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8855\n",
      "Recall/Sensitivity: 0.8187\n",
      "Specificity: 0.9402\n",
      "Precision: 0.918\n"
     ]
    }
   ],
   "source": [
    "# additional metrics\n",
    "print('Accuracy:',  round((tp + tn) / (tp+fp+tn+fn), 4))\n",
    "print('Recall/Sensitivity:', round((tp) / (tp+fn), 4))\n",
    "print('Specificity:', round((tn) / (tn+fp), 4))\n",
    "print('Precision:', round((tp) / (tp+fp), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our models were more successful than the baseline in classifying the subreddit, but our TFIDF Vectorized Logistic Model outperformed the rest with an testing accuracy score of 88.68% and training score of 94.95%. We are successful in building a model that can reasonably classify a title of a reddit post to belonging to subreddit 'teslamotors' or 'cars'. We were also successfull in identifying key words beyond the make and model that hold significance, such as autopilot, supercharger, and fsd for full self driving. Tesla spends $0 on advertising, so it is not surprising if the population is not familiar with these features in relation to vehicles. If we can educate the public on new technologies that exist on Tesla vehicles, specifically context behind the highest non-make and model coefficients of our model from subreddit 'teslamotors', we can potentially reduce the dependency on gas, slow down global warming and pollution, and save some lives with autopilot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
