{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Alex Lau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automobiles have progressed slowly over the last century. Most cars on the road today are still powered by gasoline, a non-renewable resource that pollutes the air and contributes to global warming. There is one manufacturer that is revolutionizing transportation with electric vehicles: Tesla Motors. We have decided to investigate subreddit titles from a popular American social news website, Reddit, to better understand the public sentiments surrounding Tesla. Our goal is to correctly classify the titles of posts as originating from subreddit 'cars' or 'teslamotors'. We plan to solve this problem using Logistic Regression and Multinomial Naive Bayes Classifier. Our models will be evaluated using Accuracy Scores. As Data Scientists of the 21st century, it is important that we leverage what we learn from this process to drive the population to transition to clean energy sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-4 Paragraphs (NOT bullet points)\n",
    "\n",
    "__BROAD__ overview of workflow.\n",
    "- What did you start with _(What's your data? Where did you get it?)_\n",
    "- What did you do with it _(Brief overview of data cleaning and EDA process - don't get too into the weeds describing your cleaning)_\n",
    "- Process for finding best model\n",
    "\n",
    "Here's what I found. Here are some of the key takeaways and interesting findings. _(this should flow naturally into your conclusions & recommendations)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Problem Statement](#1.-Problem-Statement)\n",
    "2. [Executive Summary](#2.-Executive-Summary)\n",
    "3. [Table of Contents](#3.-Table-of-Contents)\n",
    "4. [Loading Libraries & Data](#4.-Loading-Libraries-&-Data)\n",
    "5. [Web Scraping](#5.-Web-Scraping)\n",
    "6. [Exploratory Data Analysis (EDA)](#6.-Preliminary-EDA)\n",
    "7. [Data Cleaning](#7.-Data-Cleaning)\n",
    "8. [Model Preperation (Preprocessing)](#9.-Model-Preparation-(Preprocessing))\n",
    "9. [Modeling](#10-Modeling)\n",
    "    <br>9.1 [Baseline Model](#9.1-Baseline-Model)\n",
    "    <br>9.2 [Logistic Regression](#9.2-Logistic-Regression)\n",
    "    <br>9.3 [Gaussian Naive Bayes](#9.3-Gaussian-Naive-Bayes)\n",
    "10. [Model_Selection](#11.-Model-Selection)\n",
    "11. [Model Evaluation](#12.-Model-Evaluation)\n",
    "12. [Conclusions and Evaluation](#13.-Conclusions-and-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime as dt\n",
    "import time\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're asking pushshift to engage with redit website\n",
    "def query_pushshift(subreddit, kind='submission', skip=30, times=5, \n",
    "                    subfield = ['title', 'selftext', 'subreddit', 'created_utc', 'author', 'num_comments', \n",
    "                                'score', 'is_self'],\n",
    "                    comfields = ['body', 'score', 'created_utc']):\n",
    "    stem = \"https://api.pushshift.io/reddit/search/{}/?subreddit={}&size=500\".format(kind, subreddit)\n",
    "    \n",
    "    mylist = []\n",
    "    \n",
    "    for x in range(1, times + 1):\n",
    "        \n",
    "        URL = \"{}&after={}d\".format(stem, skip * x)\n",
    "        print(URL)\n",
    "        response = requests.get(URL)\n",
    "        assert response.status_code == 200\n",
    "        mine = response.json()['data']\n",
    "        df = pd.DataFrame.from_dict(mine)\n",
    "        mylist.append(df)\n",
    "        time.sleep(2)\n",
    "        \n",
    "    full = pd.concat(mylist, sort=False)\n",
    "    \n",
    "    if kind == \"submission\":\n",
    "        \n",
    "        full = full[subfield]\n",
    "        \n",
    "        full = full.drop_duplicates()\n",
    "        \n",
    "        full = full.loc[full['is_self'] == True]\n",
    "        \n",
    "    def get_date(created):\n",
    "        return dt.date.fromtimestamp(created)\n",
    "    \n",
    "    _timestamp = full[\"created_utc\"].apply(get_date)\n",
    "    \n",
    "    full['timestamp'] = _timestamp\n",
    "    print(full.shape)\n",
    "    \n",
    "    return full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=30d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=60d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=90d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=120d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=150d\n",
      "(1347, 9)\n"
     ]
    }
   ],
   "source": [
    "sub_1_query = query_pushshift('teslamotors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=30d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=60d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=90d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=120d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=150d\n",
      "(1648, 9)\n"
     ]
    }
   ],
   "source": [
    "sub_2_query = query_pushshift('cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sub_queries = pd.concat([sub_1_query, sub_2_query])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2995, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           object\n",
       "selftext        object\n",
       "subreddit       object\n",
       "created_utc      int64\n",
       "author          object\n",
       "num_comments     int64\n",
       "score            int64\n",
       "is_self           bool\n",
       "timestamp       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First long road trip with our Model 3! Some th...</td>\n",
       "      <td>We got our LR RWD Model 3 back in April 2018. ...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577768864</td>\n",
       "      <td>LanFeusT23</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When do you file the 8936 EV tax credit form?</td>\n",
       "      <td>I picked up my Model 3 back in August this yea...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577771949</td>\n",
       "      <td>isoplayer</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model S Fender Flares</td>\n",
       "      <td>Hello! I hope this exceeds the Low Quality  po...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577772342</td>\n",
       "      <td>IAmDarthMole</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mar 4, 2019 Article - Tesla Shares Will Breach...</td>\n",
       "      <td>I saved this for posterity when it was posted,...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577774293</td>\n",
       "      <td>rkeith5</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Can you buy Tesla Acceleration Update with Pay...</td>\n",
       "      <td>This is a pretty straightforward question. I g...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577775193</td>\n",
       "      <td>longlivegreenearth</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "1   First long road trip with our Model 3! Some th...   \n",
       "3       When do you file the 8936 EV tax credit form?   \n",
       "5                               Model S Fender Flares   \n",
       "8   Mar 4, 2019 Article - Tesla Shares Will Breach...   \n",
       "10  Can you buy Tesla Acceleration Update with Pay...   \n",
       "\n",
       "                                             selftext    subreddit  \\\n",
       "1   We got our LR RWD Model 3 back in April 2018. ...  teslamotors   \n",
       "3   I picked up my Model 3 back in August this yea...  teslamotors   \n",
       "5   Hello! I hope this exceeds the Low Quality  po...  teslamotors   \n",
       "8   I saved this for posterity when it was posted,...  teslamotors   \n",
       "10  This is a pretty straightforward question. I g...  teslamotors   \n",
       "\n",
       "    created_utc              author  num_comments  score  is_self   timestamp  \n",
       "1    1577768864          LanFeusT23           132      1     True  2019-12-31  \n",
       "3    1577771949           isoplayer             0      1     True  2019-12-31  \n",
       "5    1577772342        IAmDarthMole             2      1     True  2019-12-31  \n",
       "8    1577774293             rkeith5            46      1     True  2019-12-31  \n",
       "10   1577775193  longlivegreenearth             1      1     True  2019-12-31  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>How good is a GTR R34 2002 as a daily driver?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567588357</td>\n",
       "      <td>Oeirs</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>I don't want a 'normie' car.</td>\n",
       "      <td>I know this sounds pretentious as fuck, but I ...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567589149</td>\n",
       "      <td>Dyllbyll</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Need some help</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567591430</td>\n",
       "      <td>tbrits88</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Finally got my first dream car a 2013 Infiniti...</td>\n",
       "      <td>So I finally went out and got one of my first ...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567592222</td>\n",
       "      <td>NateT1015</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Reconditioned &amp;amp; used Range Rover Evoque En...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567593458</td>\n",
       "      <td>DavidRonan</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "490      How good is a GTR R34 2002 as a daily driver?   \n",
       "491                       I don't want a 'normie' car.   \n",
       "495                                     Need some help   \n",
       "497  Finally got my first dream car a 2013 Infiniti...   \n",
       "499  Reconditioned &amp; used Range Rover Evoque En...   \n",
       "\n",
       "                                              selftext subreddit  created_utc  \\\n",
       "490                                          [removed]      cars   1567588357   \n",
       "491  I know this sounds pretentious as fuck, but I ...      cars   1567589149   \n",
       "495                                          [removed]      cars   1567591430   \n",
       "497  So I finally went out and got one of my first ...      cars   1567592222   \n",
       "499                                          [removed]      cars   1567593458   \n",
       "\n",
       "         author  num_comments  score  is_self   timestamp  \n",
       "490       Oeirs             3      1     True  2019-09-04  \n",
       "491    Dyllbyll            38      0     True  2019-09-04  \n",
       "495    tbrits88             2      1     True  2019-09-04  \n",
       "497   NateT1015             0      1     True  2019-09-04  \n",
       "499  DavidRonan             0      1     True  2019-09-04  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.995000e+03</td>\n",
       "      <td>2995.000000</td>\n",
       "      <td>2995.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.572771e+09</td>\n",
       "      <td>21.678798</td>\n",
       "      <td>13.059098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.674337e+06</td>\n",
       "      <td>84.732975</td>\n",
       "      <td>137.760808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.567397e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.570070e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.572735e+09</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.575411e+09</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.578102e+09</td>\n",
       "      <td>2962.000000</td>\n",
       "      <td>5222.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc  num_comments        score\n",
       "count  2.995000e+03   2995.000000  2995.000000\n",
       "mean   1.572771e+09     21.678798    13.059098\n",
       "std    3.674337e+06     84.732975   137.760808\n",
       "min    1.567397e+09      0.000000     0.000000\n",
       "25%    1.570070e+09      0.000000     1.000000\n",
       "50%    1.572735e+09      2.000000     1.000000\n",
       "75%    1.575411e+09     15.000000     1.000000\n",
       "max    1.578102e+09   2962.000000  5222.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            0\n",
       "selftext        19\n",
       "subreddit        0\n",
       "created_utc      0\n",
       "author           0\n",
       "num_comments     0\n",
       "score            0\n",
       "is_self          0\n",
       "timestamp        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2995 entries, 1 to 499\n",
      "Data columns (total 9 columns):\n",
      "title           2995 non-null object\n",
      "selftext        2976 non-null object\n",
      "subreddit       2995 non-null object\n",
      "created_utc     2995 non-null int64\n",
      "author          2995 non-null object\n",
      "num_comments    2995 non-null int64\n",
      "score           2995 non-null int64\n",
      "is_self         2995 non-null bool\n",
      "timestamp       2995 non-null object\n",
      "dtypes: bool(1), int64(3), object(5)\n",
      "memory usage: 213.5+ KB\n"
     ]
    }
   ],
   "source": [
    "combined_sub_queries.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       2177\n",
       "0        213\n",
       "2         77\n",
       "3         44\n",
       "5         40\n",
       "        ... \n",
       "57         1\n",
       "59         1\n",
       "61         1\n",
       "65         1\n",
       "4068       1\n",
       "Name: score, Length: 130, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HW3 Upgrades</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577775607</td>\n",
       "      <td>JohnAsHimself</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Garage door remote delete</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577794040</td>\n",
       "      <td>Satyrr</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Tesla s screen responsiveness</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577801800</td>\n",
       "      <td>nutteez</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I lost a Tesla bet so now I am paying up.</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577803979</td>\n",
       "      <td>Brad_Wesley</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Hmm</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577804855</td>\n",
       "      <td>Mako_sato_ftw</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>How good is a Skyline GTR R34 2002 as a daily ...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567586407</td>\n",
       "      <td>Oeirs</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>[Serious] With infotainment being such an inte...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567586518</td>\n",
       "      <td>NorCalAthlete</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>How good is a GTR R34 2002 as a daily driver?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567588357</td>\n",
       "      <td>Oeirs</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Need some help</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567591430</td>\n",
       "      <td>tbrits88</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Reconditioned &amp;amp; used Range Rover Evoque En...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567593458</td>\n",
       "      <td>DavidRonan</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1322 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title   selftext  \\\n",
       "11                                        HW3 Upgrades  [removed]   \n",
       "21                           Garage door remote delete  [removed]   \n",
       "30                       Tesla s screen responsiveness  [removed]   \n",
       "31           I lost a Tesla bet so now I am paying up.  [removed]   \n",
       "33                                                 Hmm  [removed]   \n",
       "..                                                 ...        ...   \n",
       "484  How good is a Skyline GTR R34 2002 as a daily ...  [removed]   \n",
       "485  [Serious] With infotainment being such an inte...  [removed]   \n",
       "490      How good is a GTR R34 2002 as a daily driver?  [removed]   \n",
       "495                                     Need some help  [removed]   \n",
       "499  Reconditioned &amp; used Range Rover Evoque En...  [removed]   \n",
       "\n",
       "       subreddit  created_utc         author  num_comments  score  is_self  \\\n",
       "11   teslamotors   1577775607  JohnAsHimself             0      1     True   \n",
       "21   teslamotors   1577794040         Satyrr             0      1     True   \n",
       "30   teslamotors   1577801800        nutteez             0      1     True   \n",
       "31   teslamotors   1577803979    Brad_Wesley            28      1     True   \n",
       "33   teslamotors   1577804855  Mako_sato_ftw             0      1     True   \n",
       "..           ...          ...            ...           ...    ...      ...   \n",
       "484         cars   1567586407          Oeirs             1      1     True   \n",
       "485         cars   1567586518  NorCalAthlete             2      0     True   \n",
       "490         cars   1567588357          Oeirs             3      1     True   \n",
       "495         cars   1567591430       tbrits88             2      1     True   \n",
       "499         cars   1567593458     DavidRonan             0      1     True   \n",
       "\n",
       "      timestamp  \n",
       "11   2019-12-31  \n",
       "21   2019-12-31  \n",
       "30   2019-12-31  \n",
       "31   2019-12-31  \n",
       "33   2019-12-31  \n",
       "..          ...  \n",
       "484  2019-09-04  \n",
       "485  2019-09-04  \n",
       "490  2019-09-04  \n",
       "495  2019-09-04  \n",
       "499  2019-09-04  \n",
       "\n",
       "[1322 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries[combined_sub_queries['selftext']=='[removed]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cars</th>\n",
       "      <td>1.572672e+09</td>\n",
       "      <td>21.541262</td>\n",
       "      <td>11.408981</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teslamotors</th>\n",
       "      <td>1.572892e+09</td>\n",
       "      <td>21.847068</td>\n",
       "      <td>15.077951</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              created_utc  num_comments      score  is_self\n",
       "subreddit                                                  \n",
       "cars         1.572672e+09     21.541262  11.408981     True\n",
       "teslamotors  1.572892e+09     21.847068  15.077951     True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider using features where comments > 20?\n",
    "combined_sub_queries.groupby('subreddit').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data up into X and y.\n",
    "X = combined_sub_queries['title']\n",
    "y = combined_sub_queries['subreddit'].map(lambda x: 1 if x == 'teslamotors' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataa into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Instantiate CountVectorizer.\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit CountVectorizer to training data.\n",
    "cv.fit(X_train)\n",
    "\n",
    "# Transform training and testing data based on the fit CountVectorizer.\n",
    "X_train_cv = cv.transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with our words.\n",
    "words = pd.DataFrame(X_train_cv.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000             13\n",
       "000rpm           1\n",
       "01               4\n",
       "02               4\n",
       "03               7\n",
       "                ..\n",
       "zero             1\n",
       "zonda            1\n",
       "zones            1\n",
       "zx4              1\n",
       "معلومه_بتهمك     1\n",
       "Length: 4192, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the      595\n",
       "to       480\n",
       "car      407\n",
       "for      312\n",
       "tesla    281\n",
       "in       281\n",
       "is       266\n",
       "and      262\n",
       "on       244\n",
       "of       229\n",
       "model    221\n",
       "my       197\n",
       "it       186\n",
       "what     181\n",
       "how      157\n",
       "with     151\n",
       "you      147\n",
       "or       114\n",
       "are      110\n",
       "cars     104\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the most frequently used words.\n",
    "words.sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the lengths of the words.\n",
    "lengths_of_words = [len(each) for each in words.columns]\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(lengths_of_words, ax=ax, kde = False)\n",
    "plt.title('Histogram of Word Lengths')\n",
    "plt.axvline(np.mean(lengths_of_words), 0,350, color = 'red')\n",
    "plt.axvline(np.median(lengths_of_words), 0,350, color = 'green')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Characters')\n",
    "ax.set_xlim(1,20)\n",
    "ax.set_xticks(range(1,20))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEgCAYAAACq+TSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3defxbVZ3/8debtqyibaHFQgsFQRQXlilQRGRzUHEBHReQn1AGrTriCDqyCMMyOgq44E/HQapFikNZRtlUFJhidWBkadlkq1QsUFlalpadofCZP84JTfNNbnOTfL/J9/t9Px+PPJKce+65n+Qm+eSecxdFBGZmZo2s0e0AzMystzlRmJlZIScKMzMr5ERhZmaFnCjMzKyQE4WZmRVyouhhkkLS3G7HYc2RNErSyZLukfRCXn/7d3gZ03K700rO589SmyQtkrSo23F0gxNFP8tf0MKDVfIHMCRN7uByJ+c2z+5Um7ZaXwJOAB4EvgWcDNxdr6KksyufjSZvc4sW3O0fsarXM61bMbRL0tzVfVeHq5HdDsAKvRF4tttBWNPeBzwN/G1E/O9q6l4CLKop2wPYHfgdMLdmWqXuxcB1wEOth2lWjhNFD4uIuv9GrWdtDDzWRJIgIi4hJYtXSDqJlCjmRsRJDeZbDixvO1KzEtz11MPqdTlIWl/SP0u6XdKTkp6S9GdJF0j6m1znJOAveZZDarowplW1tYakz0i6UdLTkp7Jjz8rqe5nQ9JBkm6S9JykJZJ+KmnjepvtkvbIyzxJ0k6SfiXp8epuNkl7Spoh6c78ep7Lr+1ESWvXWf5Jef49JB0oab6kZyU9KOk7ktbK9fbKMT0p6Ykc5wYl3//XSPqGpAWSns/tXCHpnTX1zs6vfXNgs6r3elGZ5TUZ0ypjFJX3GNisZtlNdTtKGinpHyRdl9+rZyXdLOnwRp+BDr2OdSUdK+mW/Ll7WtIfJB1Yp27152i7/DlalmP9naS3NVjGBEk/yZ/T5/KyDqluL9ebnN/D3fPzwi6/HPs3Jd2vNBa1UNLRklSn7gckzZH0UK77YI75H9p8CweUtygGkfxB/A3wNuAPwI+BFcAkUrfFfwPzSd0Wo4EvALey6j/XW6oe/xT4OPBAbiuADwL/DrwdOKhm+V8GTgOeAGaR/tn+LXAtxf9ydwGOBa4BzgI2BCr/uo8G3gD8D/ArYG1gV+AkYA9J74yIl+q0+XngPfm1zQX2AY4Exkq6FDg/tzcjv1//Ly/3PQVxVr/W0fl1bQPcCHw3z/9R4EpJn42IM3P1SjfSEfn5d/P9smaW1aZFpLGQ2mXDquu6D0mjgF8A7wIWALOB54E9ge8DOwOf6Gy4r7y3VwPbAzeRPhNr5DhmS3pTRBxfZ9YpwFGs/OxvCvwdMEfSdhGxoGoZ40mfqcnA7/Pj15I+21fWtLuM9B5OIyXck6umLaqpOyrPvzHwa9L3b3/gFNJn95V5JU0HzgQeJr3PjwLjgbcCh+ZYBoeI8K0fb6Qf3yD98DW6Lct1JteZd27V87fksovrLGcNYEzV88m57tkN4jowT78JeFVV+XrAvDzt41XlWwAvAkuBSVXlAs6rvM6aZexR9fo/3SCOLQDVKf9qnu9jNeUn5fLlwBurytcC7gBeAh4Ddq95b67K823X5Ho7M9c/szo+YKu87BfqrK9FwKI2PiuV13ZSQZ1puc60Msuu/SzVLO/7wIiq8hHAzDxtvyZjP7teXKupe1RN+dqkP0IvV6+nms9R7ev+dC7/95rySvyn1pRvm9ddn/eZ9IcjCuJelOe7HFinqnw86Tu8DBhVVT4/L2t8nbY2bPVz0o1b1wMY6reqD3gzt8l15p1b9bySKGY3sdzJFCeKyg/nPnWm7Z2nXV1VdnwuO6FO/c1I/6yiprzyBb+5hfdtgzzvWTXllR+3r9aZ54Q87Zw60w7J0w5pYtmjgGeAp4CxdaZXktgJNeWLGCSJgpQ8HyUNio+sU3806Qf7wiZjP7teXA3W6wrgxgbTt83tnFbnc3RNg3X1IjCvqmxN0k4gy4D168zzo3rvM80nii3rTJuVp725qmx+/hyNadTmYLm562mARESf/suK3Je9WRPN3EnqTjhQ0mbApaTunHnRxABqjR1IPwRz60z7Hemf+fZVZZXH19RWjoj7JD1ASk713NAoCEnrkbrIPgi8HliftJVSsUmDWefVKXsw38+vM+2v+X5io1iqvAFYF7g2Ih6vM/1qUuLcvs60weL1pB/te4Dj63SvAzxH2vOuk3YkbbG8MkZQY1S+r7fcPus8Il6U9Agwpqp4a2Ad0vfiqTrtXAN8skzQVZZHxMI65Q/k++o4zgW+Ddwh6QLS9+raiFja4rK7xoliEImIlyTtRfrn/GHg1DzpKUmzgGMj4ukmm3sN8Hi9BBMRKyRV+lOr6wM80qC9R2icKB6uV5j7yK8GdgJuBy4gdW29mKucSOpSqqfemMiKJqaNqjOtVuW1NtoFtVI+uom2elVlYH8r0vvcyKv6abk75luZ5TYa81lBSj4VzXxWW1UUA9VxRMR38vfoH4B/JI0jhaTfAV+OiHp/dnqS93oaZCLiiYg4MiImkb7knyQd1HU4cEaJppaTBn77/HBKGkkauH2yqrjyeKMG7TUqh7RJXs9+pCQxKyLeEhHTI+K4SLuGntlgnoFQSTSvbTB9Qk29wagS+8URoYLb5v203NNXs9w921hGO5/VjoqIcyJiKilBvpc0dvIO4Io84D4oOFEMYhGxMCJmknbre5r0w1tR2VNoRJ8Zk5tJ6/8ddaa9I893U019SHtDrSJ3g01qPvJXbJnvf15n2u4ttNcpC0h93NtJGlNneuVH7KY607rlJRqv63ruJv07nlrvz0I/uoHU5blbPy7jblK32VslrV9nep/PcPYSgKQy72NTImJZRFweEZ8ijeeMpX/fg45yohhEJG0u6U11Jo0hddE8V1X2BOmf/KYNmjsr339D0rpVy1iXtKsfpH8/FbNJm9eflzSpqr6Ab1DuR6piUb7fo7pQ0has7FYbcLk77lxS98e/VE+T9DpSN8KLpN2Le8VjwDhJ6zRTOSJWkPZ2mgB8r958+TiEbToZZEQsIb23U5SOB+rT/S3pdZJa3pLJ6+8CUhfUKrvZStoWOLjBrI/l+0bfmVIkvbve62Nll+6gOeuCxygGl22BiyXNJ/XpPwiMI21JjKLqxzUinpZ0PbCbpHOBP5H+MV0WEbdFxGxJ+5GOC7hD0iWkxLI/6cCxCyPi3Kr2/izpBODrwK15cK5yHMVY0vEaby35en4BLAS+KOktpK2WTUmnwvgVHfrCtugY0j++wyXtCPyWlcdRrA8cHhF/6WJ8teaQ+vx/I+n3pN0yb42IXxTM81XSZ+ozwPslXU0a9B9P6tbcFTiOtBNFsz4paY8G02ZHxJWkbtKtSEn4E5KuIY0bbEwaxN6RtPt2O+/vMcBewFGSdiYdRzGBtP4uJ33OX66ZZw7wEeAiSZeT/njdFxGt/iE4H3g+v75FpJ00diO9vvnAf7XY7oBzohhc5pH+ve8OvJu0JbGU9KH7XkT8uqb+J4DTc90DSR/UxcBtefqBpD0x/p60PzrAXaQ9NfqMd0TENyQtBr5IOmDoKeAK0kFQV7LqmMZqRcQzeXD+FNJWxW7AvaQfsO8AHyvTXidFxOOSKgcKfoj0mp8jdZ18M//g9ZKvkQbX30/6gR9B2mWzYaLIewztTzoYcRopQb+K9Jn6C/DPpH//Zeyab/XcAlwZEU9K2h2YTjrg8+9Ix1A8QtoL60jS7tsti4hH8hHbXwf2JR08uIA0sPwMKVHUfl5/TNr78ADSZ3ok6fvRaqI4hnQQ4Q45hueB+0gHmZ4RES8WzNtTlPf3NWuZpFeTvuS3RMQu3Y7HrIikfwW+Arw7Iq7odjyDgccorGmSxtUOfOY+2G+T/hFe3JXAzOqQtHGdsreQxpgeJ20tWBPc9WRl/B3wL5L+i3SA0VjSHlKvJ3UrfL+LsZnVmidpIWk87xnSuMh7SX+QPxMRz3czuMHEXU/WNEnbk/qtd2LlgVN/AS4inVOn3lGwZl0h6UTSWMRk0g4Iy0jX8vhWRMztXmSDjxOFmZkVGnJdTxtuuGFMnjy522G0bMFj6UzJW2+w9Woq5jMqb72aemZmTZg/f/6jETGu3rQhlygmT57MvHmD5hQqfexx9h4AzJ02dzUVUz3mrqaemVkTJN3XaJr3ejIzs0JOFGZmVsiJwszMCjlRmJlZIScKMzMr5ERhZmaFnCjMzKyQE4WZmRVyojAzs0JD7sjsXjL7+vubrvvxnbt5MTczs8YGfItC0iJJf5R0i6R5uWyspKsk3ZPvx+RySfqepIWSbpO0w0DHa2Y23HWr62nPiNguIqbk58cAcyJiK9J1a4/J5e8hnUN+K9JlE/tcntPMzPpXr4xR7Ee6vi/5fv+q8nMiuQ4YLWlCNwI0MxuuupEoArhS0nxJ03PZRhHxEEC+H5/LNyFdSa1icS5bhaTpkuZJmrd06dJ+DN3MbPjpxmD2rhHxoKTxwFWS7i6oqzplfa60FBEzgBkAU6ZM8ZWYzMw6aMC3KCLiwXy/BLiYdFnNRypdSvl+Sa6+GJhUNftE4MGBi9bMzAY0UUhaT9L6lcfAPqQLn18GHJKrHQJcmh9fBhyc936aCiyvdFGZmdnAGOiup42AiyVVlj07In4j6UbgQkmHAfcDH8n1Lwf2BRYCzwKHDnC8ZmbD3oAmioi4F9i2TvljwN51ygP43ACEZmZmDfTK7rFmZtajnCjMzKyQE4WZmRXySQFLKHOSPzOzocJbFGZmVshbFD2isrWy5MkXVnlej09JbmYDyVsUZmZWyInCzMwKOVGYmVkhJwozMyvkRGFmZoWcKMzMrJAThZmZFXKiMDOzQk4UZmZWyInCzMwKOVGYmVkhJwozMyvkRGFmZoWcKMzMrJAThZmZFXKiMDOzQk4UZmZWyInCzMwKOVGYmVkhJwozMyvkRGFmZoWcKMzMrJAThZmZFXKiMDOzQk4UZmZWyInCzMwKdSVRSBoh6WZJv8zPN5d0vaR7JF0gac1cvlZ+vjBPn9yNeM3MhrNubVF8Abir6vmpwOkRsRXwBHBYLj8MeCIitgROz/XMzGwADXiikDQReC/w4/xcwF7Az3KVWcD++fF++Tl5+t65vpmZDZBubFF8FzgKeDk/3wBYFhEr8vPFwCb58SbAAwB5+vJcfxWSpkuaJ2ne0qVL+zN2M7NhZ0AThaT3AUsiYn51cZ2q0cS0lQURMyJiSkRMGTduXAciNTOzipEDvLxdgQ9I2hdYG3g1aQtjtKSReathIvBgrr8YmAQsljQSeA3w+ADHbGY2rA3oFkVEHBsREyNiMnAAcHVEHAT8FvhwrnYIcGl+fFl+Tp5+dUT02aIwM7P+0yvHURwNfFHSQtIYxMxcPhPYIJd/ETimS/GZmQ1bA9319IqImAvMzY/vBXaqU+d54CMDGpiZma2iV7YozMysRzlRmJlZIScKMzMr5ERhZmaFnCjMzKyQE4WZmRVyojAzs0JOFGZmVsiJwszMCjlRmJlZIScKMzMr5ERhZmaFSiUKSedL2seXIzUzGz7KblFMAn4D3C/pa5K27IeYzMysh5RKFBGxK7A18FPgYGCBpN9LmiZpvf4I0MzMuqv0GEVE3BMRXwE2A/YlXa70B8BDkmZKenuHYzQzsy5q+cJFERGSfg+MB7YEppASx6GSbgb+PiJu7UyYVm329fez95MvADDn+vsL6358500HIiQzG8Ja2utJ0q6SfgQ8DHwfuAXYJSImANsBT5K6p8zMbJArtUUh6VhgGmkL4g/AEcAFEfFspU5E3CbpeOD3HYzTzMy6pGzX0xeAc4CZEbGgoN7dwPSWozIzs55RNlFMjIgVq6sUEY8BM1sLyczMeknZMYq3Szq43gRJn5C0ewdiMjOzHlI2UXwd2LjBtNfm6WZmNoSUTRRvBuY1mHYT8Kb2wjEzs15TNlG8DIxpMG2DFtozM7MeV/aH/VrgS5JGVRfm50cC13QqMDMz6w1l93r6CikZ3CPpPOAhYAJwADAW2K2z4ZmZWbeVShQRcaukqcBJwKdI3VBPAHOAEyPi7o5HaGZmXVX6XE8RcQfwkX6IxczMepAHn83MrFDpLQpJ+wMfAiYCa9dOj4i3dSAuMzPrEWUvhfrPwEXA9sBS4M91bkXzry3pBkm3SrpD0sm5fHNJ10u6R9IFktbM5Wvl5wvz9MllX6CZmbWn7BbFdOCbEXF0i8t7AdgrIp7Ou9ReI+nXwBeB0yPifEk/BA4Dzsj3T0TElpIOAE4FPtbiss3MrAVlxyjWB65sdWGRPJ2fjsq3APYCfpbLZwH758f75efk6XtLUqvLNzOz8somiguBfdpZoKQRkm4BlgBXkbqrllWdlXYxsEl+vAnwAECevpx0BHhtm9MlzZM0b+nSpe2EZ2ZmNcp2Pf0G+JaksaQf+WW1FSKicIsjIl4CtpM0GrgYeGO9avm+3tZD9CmImAHMAJgyZUqf6WZm1rqyiaLSPXRYvtUKYEQzDUXEMklzganAaEkj81bDRODBXG0xMAlYLGkk8Brg8ZIxm5lZG8omiq3aWZikccCLOUmsA7yTNED9W+DDwPnAIcCleZbL8vM/5OlXR4S3GMzMBlDZU3gU7v7ahAnALEkjSOMjF0bELyXdCZwv6WvAzay8Ot5M4KeSFpK2JA5oc/lmZlZSKwfcjQKmAVNI3UL/GBELJX0Y+GPRtbQj4jbSMRi15fcCO9Upfx6fLsTMrKtKJQpJW5J2j92QdKGi3YBX58l7Au8ndRWZmdkQUXb32O8BDwOTSeML1Xsl/Q6fZtzMbMgp2/W0O/DRiHg8jzNUe5g0BmFmZkNI2S2KF4C1GkzbmDrHVZiZ2eBWNlFcBRwraf2qssgD3IeTDsgzM7MhpGzX05eB/wEWAleQDrA7DngTsB7w0Y5GZ2ZmXVf2OIr7JW0L/BOwN3AfaWD7MuBbETHoTrQ0+/r7ux2CmVlPa+VSqI8Bx/ZDLGZm1oN8KVQzMytU9oC7h6hz9tZqEbFxWxGZmVlPKdv1NJO+iWIs6cJD67LyIkNmZjZElB3MPr5euaQ1gP8Enu1EUGZm1js6MkYRES8DPwL+sRPtmZlZ7+jkYPZmwJodbM/MzHpA2cHs6XWK1yRdzvRg4KJOBGVmZr2j7GD2D+uUrQD+Sup6OqHtiMzMrKeUTRSjagsi4qUOxWJmZj2o7F5PTgpmZsNM2TGKj5epHxGzy4VjZma9pmzX03+w8oC76qvbNSpzojAzG+TK7h67M+mMsScDbwVem+//JZfvDIzJt7GdC9PMzLql7BbFqcAZEfHNqrIlwO2SngVOi4g9OxadmZl1XdktiqnArQ2m3UbaojAzsyGk7BbFYmAacGWdadNIx1NYDylzYaaP77xpP0ZiZoNV2URxPDBb0jakq9otAcYDHwDeAhzY2fDMzKzbyh5HcaGkRcAxwKHARsAjwI3ApyPi+o5HaGZmXdXKpVBvAD7UD7GYmVkPaunssZJeI2kXSR+VNDqX9Tm9h5mZDX6lEoWkNSR9nTRofS1wHrBFnnyZpBM7HJ+ZmXVZ2S2KfwU+BxwJvJ5Vj8S+hDSobWZmQ0jZMYpDgGMi4keSRtRM+zPwus6EZWZmvaLsFsUY4J4G00YBtcnDzMwGubKJ4g7g/Q2mvQu4uWhmSZMk/VbSXZLukPSFXD5W0lWS7sn3Y3K5JH1P0kJJt0naoWS8ZmbWprJdT18HLpS0FvCfpDPEvlnS+4HPAvuvZv4VwJci4iZJ6wPzJV1FOqp7TkScIukY0nEaRwPvAbbKt52BM/BpQszMBlSpLYqIuIh0bez3AleRBrPPBj4NHBoRv17N/A9FxE358VPAXcAmwH7ArFxtFisTzn7AOZFcB4yWNKFMzGZm1p5WDribLek84I3AhsDjwJ0R8XKZdiRNBrYHrgc2ioiHcvsPSRqfq20CPFA12+Jc9lBNW9OB6QCbburzFZmZdVLTWxSS1pZ0p6R35X/4d0bE7yPi9haSxKuAnwNHRMSTRVXrlEWfgogZETElIqaMGzeuTChmZrYaTSeKiHietAXR54e6jHwE98+Bc3NXFsAjlS6lfL8kly8GJlXNPhF4sJ3lm5lZOWX3ejqPNEbREkkCZgJ3RcR3qiZdRjpGg3x/aVX5wXnvp6nA8koXlZmZDYyyYxR/Bj4s6TrgctKZY6u3MCIiflQw/67AJ4A/Sroll30FOIW0N9VhwP3AR/K0y4F9gYXAs6Qz1pqZ2QAqmyi+m+8nADvVmR5Aw0QREddQf9wBYO869YN0yhAzM+uSsonCZ4g1MxtmVjtGIelKSVsDRMRLEfESsDuwduV59a2/AzYzs4HVzGD2O4HXVJ7kkwFeBWzdX0GZmVnvKH3AXdZonMEGsdnX39903Y/v7AMbzYaLlq5wZ2Zmw0eziaLeQXZtHXhnZmaDQ7NdT1dIWlFTNqdOGRExvrbMzMwGr2YSxcn9HoWZmfWs1SaKiHCiMDMbxjyYbWZmhZwozMyskBOFmZkVcqIwM7NCThRmZlbIicLMzAo5UZiZWSEnCjMzK+REYWZmhZwozMyskBOFmZkVcqIwM7NCThRmZlao1Uuh2jDny6aaDR/eojAzs0JOFGZmVsiJwszMCjlRmJlZIScKMzMr5ERhZmaFnCjMzKyQE4WZmRUa0EQh6SxJSyTdXlU2VtJVku7J92NyuSR9T9JCSbdJ2mEgYzUzs2SgtyjOBt5dU3YMMCcitgLm5OcA7wG2yrfpwBkDFKOZmVUZ0EQREb8HHq8p3g+YlR/PAvavKj8nkuuA0ZImDEykZmZW0QtjFBtFxEMA+X58Lt8EeKCq3uJc1oek6ZLmSZq3dOnSfg3WzGy46YVE0YjqlEW9ihExIyKmRMSUcePG9XNYZmbDSy8kikcqXUr5fkkuXwxMqqo3EXhwgGMzMxv2eiFRXAYckh8fAlxaVX5w3vtpKrC80kVlZmYDZ0CvRyHpPGAPYENJi4ETgVOACyUdBtwPfCRXvxzYF1gIPAscOpCxmplZMqCJIiIObDBp7zp1A/hc/0ZkZmar0wtdT2Zm1sOcKMzMrJCvmW39ztfXNhvcvEVhZmaFnCjMzKyQE4WZmRVyojAzs0JOFGZmVsh7PVlPKbOHFHgvKbOB4C0KMzMr5ERhZmaFnCjMzKyQE4WZmRUakoPZZQdEzcysMW9RmJlZIScKMzMr5ERhZmaFhuQYhQ0fPoW5Wf/zFoWZmRVyojAzs0JOFGZmVsiJwszMCjlRmJlZIScKMzMr5N1jbdjwrrRmrfEWhZmZFXKiMDOzQk4UZmZWyGMUZnV4PMNsJW9RmJlZIW9RmLXJWx+9xeuj85wozAaQf8RsMOr5RCHp3cD/B0YAP46IU7ocktmA6K9L+pZNQL1waWEnzdZ0at319BiFpBHAD4D3ANsAB0raprtRmZkNL72+RbETsDAi7gWQdD6wH3BnV6MyG8R6YQuhrP6KeTC+F93Q64liE+CBqueLgZ1rK0maDkzPT184aOpmt/dDLBsCj/ZDu3XbPojNmptzamG9/op5QN+LYdpuf7Y92Nrtz7YHW7v92XbDH5NeTxSqUxZ9CiJmADMAJM2LiCkdD6Sf2u3Ptgdbu/3Z9mBrtz/bHmzt9mfbg63d/m67kZ4eoyBtQUyqej4ReLBLsZiZDUu9nihuBLaStLmkNYEDgMu6HJOZ2bDS011PEbFC0uHAFaTdY8+KiDtWM9uMfgqnv9rtz7YHW7v92fZga7c/2x5s7fZn24Ot3f5uuy5F9OnyNzMze0Wvdz2ZmVmXOVGYmVmhIZUoJL1b0gJJCyUd00Y7kyT9VtJdku6Q9IVcPlbSVZLuyfdjWmx/hKSbJf0yP99c0vW53QvywH3ZNkdL+pmku3Pcu3Qw3iPz+3C7pPMkrd1KzJLOkrRE0u1VZXVjVPK9vC5vk7RDC21/M78ft0m6WNLoqmnH5rYXSHpXmXarpv2TpJC0YdmYG7Ur6fM5pjsknVY23oL3YjtJ10m6RdI8STu1EHOp70WzbRe029b6a9Ru1fSW1l9Ru+2uv4L3ou3115aIGBI30mD3n4EtgDWBW4FtWmxrArBDfrw+8CfSKUROA47J5ccAp7bY/heB2cAv8/MLgQPy4x8Cn22hzVnAJ/PjNYHRnYiXdNDjX4B1qmKd1krMwDuAHYDbq8rqxgjsC/yadCzNVOD6FtreBxiZH59a1fY2+fOxFrB5/tyMaLbdXD6JtJPFfcCGZWNuEO+ewH8Ba+Xn48vGW9D2lcB7quKc20LMpb4XzbZd0G5b669Ru+2uv4J4215/BW23vf7auXW8wW7dgF2AK6qeHwsc26G2LwX+FlgATKhaoQtaaGsiMAfYC/hlXsGPVn0hVnkdTbb5atKPuWrKOxFv5ej4saS95H4JvKvVmIHJrPoDVjdG4EzgwHr1mm27ZtoHgXPrfTbyD8YuZdoFfgZsCyxi5Q9NqZjrvBcXAu+sU69UvA3avgL4WH58IDC71fe52e9Fq21X2u3U+qvXbifWX533oWPrr07bHV9/ZW5Dqeup3uk+Nmm3UUmTge2B64GNIuIhgHw/voUmvwscBbycn28ALIuIFfl5K3FvASwFfqLUpfVjSet1It6I+CvwLeB+4CFgOTC/AzFXNIqx0+vz70n/vNpuW9IHgL9GxK01k9qN+fXAbkpder+TtGOH2gU4AvimpAdI6/PYdtpu8ntRuu2adqu1tf6q2+3k+quJt6Prr6btjq6/soZSomjqdB+lGpReBfwcOCIinmynrdze+4AlETG/urhO1bJxjyR1NZwREdsDz5C6ANqW+5v3I20ybwysRzqbb61O72fdsfUp6ThgBXBuu21LWhc4Djih3uRW281GAmNIXQhfBi6UpA60C/BZ4MiImAQcCczM5aXbLvG9KNV2o3bbXX/V7eZ2OrL+6sTbsfVXp+2Orb9WDKVE0dHTfUgaRVpR50bERbn4EUkT8vQJwJKSze4KfEDSIuB8UvfTd4HRkioHP7YS92JgcURU/oX9jJQ42o0X4J3AXyJiaUS8CFwEvK0DMVc0irEj61PSIT0JVZgAAAapSURBVMD7gIMib5u32fbrSEnz1rweJwI3SXptB2JeDFwUyQ2krc4NO9AuwCGkdQfwn6QzM1eW2XTbJb8XTbfdoN2211+ddjuy/hrE25H116Dtjqy/Vg2lRNGx033kfwEzgbsi4jtVky4jrTDy/aVl2o2IYyNiYkRMzvFdHREHAb8FPtxGuw8DD0jaOhftTToVe1vxZvcDUyWtm9+XStttxVylUYyXAQfnvTqmAssr3RvNUrro1dHAByLi2ZplHiBpLUmbA1sBNzTTZkT8MSLGR8TkvB4XkwYfH+5AzJeQ/jwg6fWknRIebSfeKg8Cu+fHewH35MdNx9zC96Kpthu12+76q9duJ9ZfwfvQ9voraLvt9deWTg96dPNG2gPgT6S9Co5ro523kzbfbgNuybd9SeMJc/JKmgOMbWMZe7Byr6ctSB+chaR/C2u10N52wLwc8yWkTeCOxAucDNwN3A78lLT3RumYgfNI4xwvkr6ghzWKkbRJ/YO8Lv8ITGmh7YWk/tvKOvxhVf3jctsLyHuTNNtuzfRFrBwMbTrmBvGuCfxHfp9vAvYqG29B228njS3dSurz/psWYi71vWi27YJ221p/jdptd/0VxNv2+itou+31187Np/AwM7NCQ6nryczM+oEThZmZFXKiMDOzQk4UZmZWyInCzMwKOVHYkCTpQ5KulrRM0guS/iTpa5I2lDRZ6ayh7+t2nKsjaR9JR3Q7DhvenChsyJH0bdKxHfcCnyCdhfR04P3Aj7oYWiv2IZ16wqxrevqa2WZlSXo/6TTuh0XEWVWTfidpBumHt79jWCcinuvv5bRC0toR8Xy347DBxVsUNtQcCdxUkyQAiIiXIuLXVUXrSjpT0nJJiyWdLOmV74SkN0g6X9IDkp5VupDMETV19sjdWO+SdJmkp4F/y9O+JOnG3P4jkn4hacvauCR9UNINkp6T9JikyyVtJukk4EvAZnkZIensqvnernSW0mfzfD+StH7V9Gl5np0kzZX0HOlkddUX0nk+x/YbpfMdmfXhLQobMpROpvY24NtNznIa6eRrHyadw+oE4A7SdQUgna55AemspU+RTpNyMrAO8I2atmYCPyGd5LHyj30iKWncR7pmyGeAayW9PiKW55g/AZxDOknkV0mnZNgLGAf8mHReoL1I12OAdDp5JO1KOl3GJTn+DYBTSKduqZyDq+I84Iwc+zJJBwNfIZ1H6Y48716kMwOb9dUf5wXxzbdu3IDXks6T8+nV1Juc651TU34LcH6DeUT6Y/UV4N6q8j1yW6evZpkjSAnmKeDgXLYG8FfSGUcbzfctYFGd8v8GfltTtleO5c35+bT8/As19f4N+Hm315dvg+fmricbipo9gdmVNc/vJG0FAKk/P3dHLQReIJ1k71+BzbXyFOsVv6ptXNJUpWtIP0a6DsKzwKtIF7gB2Jp0jY+fNBlvpd11SVcVvFDSyMoNuCbH+Derie0WYN/82naSNKLM8m34caKwoeQx0g/6pk3WX1bz/H+Btauenwr8EzCDdAbPHYGv5Wlrrzorj1Q/kbQpKREJ+DTpWiQ7kq7VUJl3g3xf9rTQY0hbKP9OSgyV2wvAKFa9PkGf2ICzSFtGHyWdifQRSV91wrBGPEZhQ0ZEvCjpWtI1vY/vQJMfAb4fEadVCiS9t9Hia56/G1gX2C8insnzjiRde7zisXw/oWRcy/LyTgIurzO99sI1q8QWES+Tdhc+XdIk4CDSltJfgR+WjMWGAW9R2FDzXWCK0pXRViFpjXwxnGatQ/qXXpl/BOmCU83O+zKpy6nio6z652wB6ce5T6xVardyyInnOmDriJhX59b0Fc4i4oGIOIV07Ydtmp3PhhdvUdiQEhG/kPQdYGbeM+hS4GngDaS9jhaRdqFtxlXA5/IYxePA50gXbWrG1aTuoZ9Imgm8idSN9Up3V0S8LOko4FxJ55L2TgrSoPR5ETGPdMGojSRNI10Q59GIWAQcBcyR9DLp0rdPkbrc3ku6aNefGgUm6cz8eq4DlgN7kvauOrrJ12bDjBOFDTkR8SVJ/wMcDswm/btfRLps5LfoO77QyOdJXTE/AJ4DZgEXk8YsVhfDHyUdCpxI2rX1VlJX1gU19WZLep50BbSfAZWthaW5yoWkH/LTSLvMzgKmRcQ1kt5B2uX1p6SkdB/wG/qOSdT6A/Ap0tjJ2qStiU9FxCWre102PPkKd2ZmVshjFGZmVsiJwszMCjlRmJlZIScKMzMr5ERhZmaFnCjMzKyQE4WZmRVyojAzs0L/B1LZQqgBOPfVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the lengths of the posts.\n",
    "lengths_of_posts = [len(each) for each in X]\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(lengths_of_posts, kde = False, bins = 30) #kde = False\n",
    "plt.title('Histogram of Title Lengths', size = 20)\n",
    "plt.axvline(np.mean(lengths_of_posts), 0,350, color = 'red')\n",
    "plt.axvline(np.median(lengths_of_posts), 0,350, color = 'green')\n",
    "plt.ylabel('Frequency', size = 15)\n",
    "plt.xlabel('Characters', size = 15)\n",
    "ax.set_xlim(1,300)\n",
    "ax.set_xticks(range(0, 300, 20))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.151919866444075"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lengths_of_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(lengths_of_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cars           1648\n",
       "teslamotors    1347\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sub_queries = combined_sub_queries.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cars           1648\n",
       "teslamotors    1347\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selftext        19\n",
       "timestamp        0\n",
       "is_self          0\n",
       "score            0\n",
       "num_comments     0\n",
       "author           0\n",
       "created_utc      0\n",
       "subreddit        0\n",
       "title            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Preparation (Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_sub_queries['title']\n",
    "y = combined_sub_queries['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing_tokenizer(str_input):\n",
    "    tokens = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    stems = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return stems\n",
    "\n",
    "# following code found on https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    \n",
    "\n",
    "class StemTokenizer:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline is __ (this value changes everytime I run the reddit script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cars           0.55025\n",
       "teslamotors    0.44975\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch score with CountVectorized features and Logistic Regression on training set is 0.9555.\n",
      "\n",
      "GridSearch score with CountVectorized features and Logistic Regression on testing set is 0.8652.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate pipeline.  \n",
    "pipe_cv = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define grid of parameters to GridSearch over.\n",
    "params_grid = {\n",
    "    'cv__tokenizer': [LemmaTokenizer()], # tried [StemTokenizer(), None] with lower accuracy test scores\n",
    "    'cv__max_features': [3950, 4000], # fine tuned to this max_features\n",
    "    'cv__stop_words': ['english'],\n",
    "    'cv__ngram_range': [(1,8)], # fine tuned to this ngram range\n",
    "    'lr__solver': ['lbfgs'] # ['liblinear', 'saga'] lower accuracy test scores\n",
    "}\n",
    "\n",
    "# GridSearch over pipeline with given grid of parameters.\n",
    "gs_cv = GridSearchCV(pipe_cv, params_grid, cv=5, scoring = 'accuracy')\n",
    "\n",
    "# Fit model.\n",
    "gs_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f'GridSearch score with CountVectorized features and Logistic Regression on training set is {round(gs_cv.score(X_train, y_train), 4)}.')\n",
    "print()\n",
    "print(f'GridSearch score with CountVectorized features and Logistic Regression on testing set is {round(gs_cv.score(X_test, y_test), 4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cv',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=3950, min_df=1,\n",
       "                                 ngram_range=(1, 8), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<__main__.LemmaTokenizer object at 0x000001E2461BE308>,\n",
       "                                 vocabulary=None)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 3950,\n",
       " 'cv__ngram_range': (1, 8),\n",
       " 'cv__stop_words': 'english',\n",
       " 'cv__tokenizer': <__main__.LemmaTokenizer at 0x1e2461be308>,\n",
       " 'lr__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch score with TF-IDF Vectorized features on training set is 0.943.\n",
      "\n",
      "GridSearch score with TF-IDF Vectorized features on testing set is 0.8745.\n"
     ]
    }
   ],
   "source": [
    "# Define Pipeline - TFIDF\n",
    "pipe_tfidf = Pipeline(steps = [('tfidfvec', TfidfVectorizer()),     # first tuple is for first step: vectorizer\n",
    "                         ('lr', LogisticRegression())        # second tuple is for second step: model\n",
    "                        ])    \n",
    "\n",
    "# Construct Grid Parameters\n",
    "hyperparams_grid = {\n",
    "                'tfidfvec__tokenizer': [LemmaTokenizer()], #StemTokenizer(), None\n",
    "                'tfidfvec__max_features': [4470],\n",
    "                'tfidfvec__ngram_range': [(1,8)],\n",
    "                'tfidfvec__stop_words': ['english'],\n",
    "                'lr__solver': ['saga'] # 'lbfgs', 'liblinear',\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "gs_tfidf = GridSearchCV(pipe_tfidf, # pipeline object replaces what we usually had as empty model class\n",
    "                 param_grid=hyperparams_grid,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy')\n",
    "\n",
    "gs_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Get best params\n",
    "print(f'GridSearch score with TF-IDF Vectorized features on training set is {round(gs_tfidf.score(X_train, y_train), 4)}.')\n",
    "print()\n",
    "print(f'GridSearch score with TF-IDF Vectorized features on testing set is {round(gs_tfidf.score(X_test, y_test), 4)}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__solver': 'saga',\n",
       " 'tfidfvec__max_features': 4470,\n",
       " 'tfidfvec__ngram_range': (1, 8),\n",
       " 'tfidfvec__stop_words': 'english',\n",
       " 'tfidfvec__tokenizer': <__main__.LemmaTokenizer at 0x1e245706d48>}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvec',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=4470,\n",
       "                                 min_df=1, ngram_range=(1, 8), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_p...\n",
       "                                 tokenizer=<__main__.LemmaTokenizer object at 0x000001E245706D48>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='saga', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutlinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch score with CountVectorized features and Multinomial Naive Bayes Classifier on training set is 0.9323.\n",
      "\n",
      "GridSearch score with CountVectorized features and Multinomial Naive Bayes Classifier on testing set is 0.8718.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate pipeline.  \n",
    "pipe_mnb = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Define grid of parameters to GridSearch over.\n",
    "params_grid = {\n",
    "    'cv__tokenizer': [LemmaTokenizer()], # tried [StemTokenizer(), None] with lower accuracy test scores\n",
    "    'cv__max_features': [3975], # fine tuned to this max_features\n",
    "    'cv__stop_words': ['english'],\n",
    "    'cv__ngram_range': [(1,8)], # fine tuned to this ngram range\n",
    "    'mnb__alpha': [1]\n",
    "}\n",
    "\n",
    "# GridSearch over pipeline with given grid of parameters.\n",
    "gs_mnb = GridSearchCV(pipe_mnb, params_grid, cv=5, scoring = 'accuracy')\n",
    "\n",
    "# Fit model.\n",
    "gs_mnb.fit(X_train, y_train)\n",
    "\n",
    "print(f'GridSearch score with CountVectorized features and Multinomial Naive Bayes Classifier on training set is {round(gs_mnb.score(X_train, y_train), 4)}.')\n",
    "print()\n",
    "print(f'GridSearch score with CountVectorized features and Multinomial Naive Bayes Classifier on testing set is {round(gs_mnb.score(X_test, y_test), 4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 3975,\n",
       " 'cv__ngram_range': (1, 8),\n",
       " 'cv__stop_words': 'english',\n",
       " 'cv__tokenizer': <__main__.LemmaTokenizer at 0x1e2470f1d48>,\n",
       " 'mnb__alpha': 1}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cv',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=3975, min_df=1,\n",
       "                                 ngram_range=(1, 8), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<__main__.LemmaTokenizer object at 0x000001E2470F1D48>,\n",
       "                                 vocabulary=None)),\n",
       "                ('mnb',\n",
       "                 MultinomialNB(alpha=1, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy score for our Gaussian Naive Bayes model is: 0.9506.\n",
      "\n",
      "Testing accuracy score for our Gaussian Naive Bayes model is: 0.8598.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate TFIDFVectorizer.\n",
    "tf = TfidfVectorizer(tokenizer = LemmaTokenizer(), max_features = 4470, ngram_range = (1,8), stop_words = 'english')\n",
    "\n",
    "# Fit vectorizer.\n",
    "tf.fit(X_train, y_train)\n",
    "\n",
    "# Transform training and testing sets.\n",
    "X_train_tf = tf.transform(X_train).todense()\n",
    "X_test_tf = tf.transform(X_test).todense()\n",
    "\n",
    "# Instantiate Gaussian Naive Bayes model.\n",
    "gnb = GaussianNB(var_smoothing = 0.63)\n",
    "\n",
    "# Fit model.\n",
    "gnb.fit(X_train_tf, y_train)\n",
    "\n",
    "# Evaluate predictions,\n",
    "print(f'Training accuracy score for our Gaussian Naive Bayes model is: {round(gnb.score(X_train_tf, y_train),4)}.')\n",
    "print()\n",
    "print(f'Testing accuracy score for our Gaussian Naive Bayes model is: {round(gnb.score(X_test_tf, y_test),4)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
