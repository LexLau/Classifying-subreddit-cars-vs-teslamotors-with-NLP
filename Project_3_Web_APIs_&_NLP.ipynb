{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Alex Lau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Table of Contents](#1.-Table-of-Contents)\n",
    "2. [Loading Libraries](#2.-Loading-Libraries)\n",
    "3. [Web Scraping/Data Collection](#3.-Web-Scraping/Data-Collection)\n",
    "4. [Exploratory Data Analysis (EDA)](#4.-Exploratory-Data-Analysis-(EDA))\n",
    "5. [Data Cleaning](#5.-Data-Cleaning)\n",
    "6. [Model Preperation (Preprocessing)](#6.-Model-Preparation-(Preprocessing))\n",
    "7. [Modeling](#7.-Modeling)\n",
    "    <br>7.1 [Baseline Model](#7.1-Baseline-Model)\n",
    "    <br>7.2 [Count Vectorized Logistic Regression](#7.2-Count-Vectorized-Logistic-Regression)\n",
    "    <br>7.3 [TF-IDF Vectorized Logistic Regression](#7.3-TFIDF-Vectorized-Logistic-Regression)\n",
    "    <br>7.4 [Multinomial Naive Bayes Classifier](#7.4-Multinomial-Naive-Bayes-Classifier)\n",
    "    <br>7.5 [Gaussian Naive Bayes Classifier](#7.5-Gaussian-Naive-Bayes-Classifier)\n",
    "8. [Model Selection](#8.-Model-Selection)\n",
    "9. [Model Evaluation](#9.-Model-Evaluation)\n",
    "10. [Conclusions and Evaluation](#10.-Conclusions-and-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import datetime as dt\n",
    "import time\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from string import ascii_uppercase\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Web Scraping/Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function below to engage with the reddit website and pull the content based on subreddit topics that we will feed in. We are requesting content from the last 150 days to gather a large enough dataset to generate significant results. We are also delaying each iteration by 2 seconds to ensure we do not overload the server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're asking pushshift to engage with redit website\n",
    "def query_pushshift(subreddit, kind='submission', skip=30, times=5, # we are pulling 30 days of posts 5 times = 150 days\n",
    "                    subfield = ['title', 'selftext', 'subreddit', 'created_utc', 'author', 'num_comments', \n",
    "                                'score', 'is_self'], #\n",
    "                    comfields = ['body', 'score', 'created_utc']):\n",
    "    stem = \"https://api.pushshift.io/reddit/search/{}/?subreddit={}&size=500\".format(kind, subreddit)\n",
    "    \n",
    "    mylist = []\n",
    "    \n",
    "    for x in range(1, times + 1):\n",
    "        \n",
    "        URL = \"{}&after={}d\".format(stem, skip * x)\n",
    "        print(URL)\n",
    "        response = requests.get(URL)\n",
    "        assert response.status_code == 200\n",
    "        mine = response.json()['data']\n",
    "        df = pd.DataFrame.from_dict(mine)\n",
    "        mylist.append(df)\n",
    "        time.sleep(2) # We will wait 2 seconds each iteration to prevent overloading the server\n",
    "        \n",
    "    full = pd.concat(mylist, sort=False)\n",
    "    \n",
    "    if kind == \"submission\":\n",
    "        \n",
    "        full = full[subfield]\n",
    "        \n",
    "        full = full.drop_duplicates()\n",
    "        \n",
    "        full = full.loc[full['is_self'] == True]\n",
    "        \n",
    "    def get_date(created):\n",
    "        return dt.date.fromtimestamp(created)\n",
    "    \n",
    "    _timestamp = full[\"created_utc\"].apply(get_date)\n",
    "    \n",
    "    full['timestamp'] = _timestamp\n",
    "    print(full.shape)\n",
    "    \n",
    "    return full "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are running our query push shift functions on our 2 reddit topics below, directly related to our problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=30d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=60d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=90d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=120d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=teslamotors&size=500&after=150d\n",
      "(1313, 9)\n"
     ]
    }
   ],
   "source": [
    "sub_1_query = query_pushshift('teslamotors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=30d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=60d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=90d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=120d\n",
      "https://api.pushshift.io/reddit/search/submission/?subreddit=cars&size=500&after=150d\n",
      "(1634, 9)\n"
     ]
    }
   ],
   "source": [
    "sub_2_query = query_pushshift('cars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are combining the content from both subreddits into a single dataframe below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sub_queries = pd.concat([sub_1_query, sub_2_query])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2947, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the length of columns and rows\n",
    "combined_sub_queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           object\n",
       "selftext        object\n",
       "subreddit       object\n",
       "created_utc      int64\n",
       "author          object\n",
       "num_comments     int64\n",
       "score            int64\n",
       "is_self           bool\n",
       "timestamp       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying the data types for all columns\n",
    "combined_sub_queries.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FSD wording changed to \"coming later this year\"</td>\n",
       "      <td>Looks like as the clock rolls over to 2020 tha...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577865335</td>\n",
       "      <td>mahkus11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature Request: battery preconditioning drivi...</td>\n",
       "      <td>Here in Europe we have many other fast chargin...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577867595</td>\n",
       "      <td>sharpfoam</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My favorite Tesla hack to date!</td>\n",
       "      <td>https://imgur.com/gallery/1kgf4WF\\n\\nUsing a f...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577867764</td>\n",
       "      <td>godloki</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s Q1, where is my model Y?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577868924</td>\n",
       "      <td>code_name_duchess_18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anyone concerned about model Y Pricing</td>\n",
       "      <td>For those that purchased the first Model 3s, t...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577868984</td>\n",
       "      <td>I_Shit_Gold_Bars</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0    FSD wording changed to \"coming later this year\"   \n",
       "2  Feature Request: battery preconditioning drivi...   \n",
       "3                    My favorite Tesla hack to date!   \n",
       "4                      It’s Q1, where is my model Y?   \n",
       "5             Anyone concerned about model Y Pricing   \n",
       "\n",
       "                                            selftext    subreddit  \\\n",
       "0  Looks like as the clock rolls over to 2020 tha...  teslamotors   \n",
       "2  Here in Europe we have many other fast chargin...  teslamotors   \n",
       "3  https://imgur.com/gallery/1kgf4WF\\n\\nUsing a f...  teslamotors   \n",
       "4                                          [removed]  teslamotors   \n",
       "5  For those that purchased the first Model 3s, t...  teslamotors   \n",
       "\n",
       "   created_utc                author  num_comments  score  is_self   timestamp  \n",
       "0   1577865335              mahkus11             3      1     True  2020-01-01  \n",
       "2   1577867595             sharpfoam            62      1     True  2020-01-01  \n",
       "3   1577867764               godloki            67      1     True  2020-01-01  \n",
       "4   1577868924  code_name_duchess_18             0      1     True  2020-01-01  \n",
       "5   1577868984      I_Shit_Gold_Bars             0      1     True  2020-01-01  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the first 5 rows\n",
    "combined_sub_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Price for a matte paint job or wrap?</td>\n",
       "      <td>I don’t know much about cars but I was wonderi...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567668656</td>\n",
       "      <td>basedwatts250</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Don't you hate when you see a car from one bra...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567671007</td>\n",
       "      <td>young_legendary</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Why do we use larger rear tires for hard accel...</td>\n",
       "      <td>Looking at cars and motorcycles(this is actual...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567674235</td>\n",
       "      <td>LMGDiVa</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Shillong Tour Packages</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567677088</td>\n",
       "      <td>travenjo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>I want to purchase a car used for family, the ...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567677421</td>\n",
       "      <td>cocola-full2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "493               Price for a matte paint job or wrap?   \n",
       "494  Don't you hate when you see a car from one bra...   \n",
       "495  Why do we use larger rear tires for hard accel...   \n",
       "498                             Shillong Tour Packages   \n",
       "499  I want to purchase a car used for family, the ...   \n",
       "\n",
       "                                              selftext subreddit  created_utc  \\\n",
       "493  I don’t know much about cars but I was wonderi...      cars   1567668656   \n",
       "494                                          [removed]      cars   1567671007   \n",
       "495  Looking at cars and motorcycles(this is actual...      cars   1567674235   \n",
       "498                                          [removed]      cars   1567677088   \n",
       "499                                          [removed]      cars   1567677421   \n",
       "\n",
       "              author  num_comments  score  is_self   timestamp  \n",
       "493    basedwatts250             7      1     True  2019-09-05  \n",
       "494  young_legendary             2      1     True  2019-09-05  \n",
       "495          LMGDiVa             5      0     True  2019-09-05  \n",
       "498         travenjo             0      1     True  2019-09-05  \n",
       "499     cocola-full2             2      1     True  2019-09-05  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the last 5 rows\n",
    "combined_sub_queries.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.947000e+03</td>\n",
       "      <td>2947.000000</td>\n",
       "      <td>2947.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.572914e+09</td>\n",
       "      <td>22.685443</td>\n",
       "      <td>15.862572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.665619e+06</td>\n",
       "      <td>90.778487</td>\n",
       "      <td>157.871881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.567499e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.570161e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.572872e+09</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.575501e+09</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.578242e+09</td>\n",
       "      <td>2962.000000</td>\n",
       "      <td>5222.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc  num_comments        score\n",
       "count  2.947000e+03   2947.000000  2947.000000\n",
       "mean   1.572914e+09     22.685443    15.862572\n",
       "std    3.665619e+06     90.778487   157.871881\n",
       "min    1.567499e+09      0.000000     0.000000\n",
       "25%    1.570161e+09      0.000000     1.000000\n",
       "50%    1.572872e+09      2.000000     1.000000\n",
       "75%    1.575501e+09     16.000000     1.000000\n",
       "max    1.578242e+09   2962.000000  5222.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewing the statistics of the columns, only limited to 3 numeric columns\n",
    "combined_sub_queries.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing self-text values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title            0\n",
       "selftext        41\n",
       "subreddit        0\n",
       "created_utc      0\n",
       "author           0\n",
       "num_comments     0\n",
       "score            0\n",
       "is_self          0\n",
       "timestamp        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# idenfifying missing values\n",
    "combined_sub_queries.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s Q1, where is my model Y?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577868924</td>\n",
       "      <td>code_name_duchess_18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FSD in Europe</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577876752</td>\n",
       "      <td>Sebbikul</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tesla Model Y in Charlotte, NC</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577878008</td>\n",
       "      <td>PikeForPresident</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Anyone else notice the Upgrades section is mis...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577879613</td>\n",
       "      <td>pmv2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Third eye</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577880250</td>\n",
       "      <td>jpteslalove</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>Delhi to Agra Cabs</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567666392</td>\n",
       "      <td>travenjo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>Do I need to press the clutch down all the way...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567667473</td>\n",
       "      <td>7sidedleaf</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Don't you hate when you see a car from one bra...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567671007</td>\n",
       "      <td>young_legendary</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Shillong Tour Packages</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567677088</td>\n",
       "      <td>travenjo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>I want to purchase a car used for family, the ...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567677421</td>\n",
       "      <td>cocola-full2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1310 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title   selftext  \\\n",
       "4                        It’s Q1, where is my model Y?  [removed]   \n",
       "11                                       FSD in Europe  [removed]   \n",
       "12                      Tesla Model Y in Charlotte, NC  [removed]   \n",
       "14   Anyone else notice the Upgrades section is mis...  [removed]   \n",
       "15                                           Third eye  [removed]   \n",
       "..                                                 ...        ...   \n",
       "484                                 Delhi to Agra Cabs  [removed]   \n",
       "488  Do I need to press the clutch down all the way...  [removed]   \n",
       "494  Don't you hate when you see a car from one bra...  [removed]   \n",
       "498                             Shillong Tour Packages  [removed]   \n",
       "499  I want to purchase a car used for family, the ...  [removed]   \n",
       "\n",
       "       subreddit  created_utc                author  num_comments  score  \\\n",
       "4    teslamotors   1577868924  code_name_duchess_18             0      1   \n",
       "11   teslamotors   1577876752              Sebbikul             0      1   \n",
       "12   teslamotors   1577878008      PikeForPresident             0      1   \n",
       "14   teslamotors   1577879613               pmv2018             0      1   \n",
       "15   teslamotors   1577880250           jpteslalove             0      1   \n",
       "..           ...          ...                   ...           ...    ...   \n",
       "484         cars   1567666392              travenjo             0      1   \n",
       "488         cars   1567667473            7sidedleaf             2      1   \n",
       "494         cars   1567671007       young_legendary             2      1   \n",
       "498         cars   1567677088              travenjo             0      1   \n",
       "499         cars   1567677421          cocola-full2             2      1   \n",
       "\n",
       "     is_self   timestamp  \n",
       "4       True  2020-01-01  \n",
       "11      True  2020-01-01  \n",
       "12      True  2020-01-01  \n",
       "14      True  2020-01-01  \n",
       "15      True  2020-01-01  \n",
       "..       ...         ...  \n",
       "484     True  2019-09-05  \n",
       "488     True  2019-09-05  \n",
       "494     True  2019-09-05  \n",
       "498     True  2019-09-05  \n",
       "499     True  2019-09-05  \n",
       "\n",
       "[1310 rows x 9 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the posts without self-texts\n",
    "combined_sub_queries[combined_sub_queries['selftext']=='[removed]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1310 or over 44% of posts have had their self-texts removed and a small portion do not have any values populated in this column. We will be focusing on the titles columns instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FSD wording changed to \"coming later this year\"</td>\n",
       "      <td>Looks like as the clock rolls over to 2020 tha...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577865335</td>\n",
       "      <td>mahkus11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature Request: battery preconditioning drivi...</td>\n",
       "      <td>Here in Europe we have many other fast chargin...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577867595</td>\n",
       "      <td>sharpfoam</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My favorite Tesla hack to date!</td>\n",
       "      <td>https://imgur.com/gallery/1kgf4WF\\n\\nUsing a f...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577867764</td>\n",
       "      <td>godloki</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s Q1, where is my model Y?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577868924</td>\n",
       "      <td>code_name_duchess_18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anyone concerned about model Y Pricing</td>\n",
       "      <td>For those that purchased the first Model 3s, t...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577868984</td>\n",
       "      <td>I_Shit_Gold_Bars</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Daily Discussion + Support Thread - January 01</td>\n",
       "      <td>Use this recurring thread for basic Q&amp;amp;A, v...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577869893</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>179</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FSD in Europe</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577876752</td>\n",
       "      <td>Sebbikul</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tesla Model Y in Charlotte, NC</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577878008</td>\n",
       "      <td>PikeForPresident</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Postponing delivery of M3P.</td>\n",
       "      <td>My delivery is estimated to be ready by around...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577879407</td>\n",
       "      <td>eklitz</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Anyone else notice the Upgrades section is mis...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577879613</td>\n",
       "      <td>pmv2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Third eye</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577880250</td>\n",
       "      <td>jpteslalove</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Transfer FSD to new Tesla at no or minimal cost</td>\n",
       "      <td>I've been expecting to trade my Model 3 in a y...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577883530</td>\n",
       "      <td>NeedMoreGrits</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Learning about Self-Driving</td>\n",
       "      <td>I'm working my way through a (free) [Self-Driv...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577886681</td>\n",
       "      <td>manx203</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019.40.2.5 + FSD (AP2 Model S) in Germany</td>\n",
       "      <td>I have received the 2019.40.2.5 update. \\n\\nI ...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577888354</td>\n",
       "      <td>SoldadoAruanda</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Buying a Tesla - From Order Page to Delivery</td>\n",
       "      <td>I made a [video](https://youtu.be/pSVEzsPkvv8)...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577889631</td>\n",
       "      <td>Venator-Fox</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Tesla's and all cars need a thank you button</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577890754</td>\n",
       "      <td>Jeweled_Peasant</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Estimated Delivery in the UK jumps from Februa...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577892127</td>\n",
       "      <td>dmy30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>UK Delivery estimate jumps from February to No...</td>\n",
       "      <td>Looking at the Model 3 design studio here in t...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577892222</td>\n",
       "      <td>dmy30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>HW2 -&amp;gt; HW3</td>\n",
       "      <td>Has anyone on the east coast been able to upgr...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577892954</td>\n",
       "      <td>w1295</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Question: I purchased the Acceleration Boost U...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577894870</td>\n",
       "      <td>Chad_Bradington</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0     FSD wording changed to \"coming later this year\"   \n",
       "2   Feature Request: battery preconditioning drivi...   \n",
       "3                     My favorite Tesla hack to date!   \n",
       "4                       It’s Q1, where is my model Y?   \n",
       "5              Anyone concerned about model Y Pricing   \n",
       "6      Daily Discussion + Support Thread - January 01   \n",
       "11                                      FSD in Europe   \n",
       "12                     Tesla Model Y in Charlotte, NC   \n",
       "13                        Postponing delivery of M3P.   \n",
       "14  Anyone else notice the Upgrades section is mis...   \n",
       "15                                          Third eye   \n",
       "18    Transfer FSD to new Tesla at no or minimal cost   \n",
       "20                        Learning about Self-Driving   \n",
       "23         2019.40.2.5 + FSD (AP2 Model S) in Germany   \n",
       "26       Buying a Tesla - From Order Page to Delivery   \n",
       "28       Tesla's and all cars need a thank you button   \n",
       "29  Estimated Delivery in the UK jumps from Februa...   \n",
       "30  UK Delivery estimate jumps from February to No...   \n",
       "31                                      HW2 -&gt; HW3   \n",
       "32  Question: I purchased the Acceleration Boost U...   \n",
       "\n",
       "                                             selftext    subreddit  \\\n",
       "0   Looks like as the clock rolls over to 2020 tha...  teslamotors   \n",
       "2   Here in Europe we have many other fast chargin...  teslamotors   \n",
       "3   https://imgur.com/gallery/1kgf4WF\\n\\nUsing a f...  teslamotors   \n",
       "4                                           [removed]  teslamotors   \n",
       "5   For those that purchased the first Model 3s, t...  teslamotors   \n",
       "6   Use this recurring thread for basic Q&amp;A, v...  teslamotors   \n",
       "11                                          [removed]  teslamotors   \n",
       "12                                          [removed]  teslamotors   \n",
       "13  My delivery is estimated to be ready by around...  teslamotors   \n",
       "14                                          [removed]  teslamotors   \n",
       "15                                          [removed]  teslamotors   \n",
       "18  I've been expecting to trade my Model 3 in a y...  teslamotors   \n",
       "20  I'm working my way through a (free) [Self-Driv...  teslamotors   \n",
       "23  I have received the 2019.40.2.5 update. \\n\\nI ...  teslamotors   \n",
       "26  I made a [video](https://youtu.be/pSVEzsPkvv8)...  teslamotors   \n",
       "28                                          [removed]  teslamotors   \n",
       "29                                          [removed]  teslamotors   \n",
       "30  Looking at the Model 3 design studio here in t...  teslamotors   \n",
       "31  Has anyone on the east coast been able to upgr...  teslamotors   \n",
       "32                                          [removed]  teslamotors   \n",
       "\n",
       "    created_utc                author  num_comments  score  is_self  \\\n",
       "0    1577865335              mahkus11             3      1     True   \n",
       "2    1577867595             sharpfoam            62      1     True   \n",
       "3    1577867764               godloki            67      1     True   \n",
       "4    1577868924  code_name_duchess_18             0      1     True   \n",
       "5    1577868984      I_Shit_Gold_Bars             0      1     True   \n",
       "6    1577869893         AutoModerator           179      1     True   \n",
       "11   1577876752              Sebbikul             0      1     True   \n",
       "12   1577878008      PikeForPresident             0      1     True   \n",
       "13   1577879407                eklitz             6      1     True   \n",
       "14   1577879613               pmv2018             0      1     True   \n",
       "15   1577880250           jpteslalove             0      1     True   \n",
       "18   1577883530         NeedMoreGrits            75      1     True   \n",
       "20   1577886681               manx203             9      1     True   \n",
       "23   1577888354        SoldadoAruanda            18      1     True   \n",
       "26   1577889631           Venator-Fox            16      1     True   \n",
       "28   1577890754       Jeweled_Peasant             0      1     True   \n",
       "29   1577892127                 dmy30             0      1     True   \n",
       "30   1577892222                 dmy30             0      1     True   \n",
       "31   1577892954                 w1295             0      1     True   \n",
       "32   1577894870       Chad_Bradington             0      1     True   \n",
       "\n",
       "     timestamp  \n",
       "0   2020-01-01  \n",
       "2   2020-01-01  \n",
       "3   2020-01-01  \n",
       "4   2020-01-01  \n",
       "5   2020-01-01  \n",
       "6   2020-01-01  \n",
       "11  2020-01-01  \n",
       "12  2020-01-01  \n",
       "13  2020-01-01  \n",
       "14  2020-01-01  \n",
       "15  2020-01-01  \n",
       "18  2020-01-01  \n",
       "20  2020-01-01  \n",
       "23  2020-01-01  \n",
       "26  2020-01-01  \n",
       "28  2020-01-01  \n",
       "29  2020-01-01  \n",
       "30  2020-01-01  \n",
       "31  2020-01-01  \n",
       "32  2020-01-01  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing documents where sefttext was removed.\n",
    "combined_sub_queries.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2947 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      "title           2947 non-null object\n",
      "selftext        2906 non-null object\n",
      "subreddit       2947 non-null object\n",
      "created_utc     2947 non-null int64\n",
      "author          2947 non-null object\n",
      "num_comments    2947 non-null int64\n",
      "score           2947 non-null int64\n",
      "is_self         2947 non-null bool\n",
      "timestamp       2947 non-null object\n",
      "dtypes: bool(1), int64(3), object(5)\n",
      "memory usage: 210.1+ KB\n"
     ]
    }
   ],
   "source": [
    "combined_sub_queries.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems people are slightly more engaging on subreddit 'teslamotors' given the higher mean number of comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cars</th>\n",
       "      <td>1.572793e+09</td>\n",
       "      <td>21.921665</td>\n",
       "      <td>16.282742</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teslamotors</th>\n",
       "      <td>1.573065e+09</td>\n",
       "      <td>23.635948</td>\n",
       "      <td>15.339680</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              created_utc  num_comments      score  is_self\n",
       "subreddit                                                  \n",
       "cars         1.572793e+09     21.921665  16.282742     True\n",
       "teslamotors  1.573065e+09     23.635948  15.339680     True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifying the mean values for each subreddit\n",
    "combined_sub_queries.groupby('subreddit').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for displaying bar graph of frequent terms based on the subreddit\n",
    "def frequent_terms(subreddit, ngrams):\n",
    "    cv = CountVectorizer(stop_words = 'english', ngram_range=ngrams)\n",
    "    cv.fit(combined_sub_queries.loc[combined_sub_queries['subreddit'] == subreddit,'title'])\n",
    "    X_cv = cv.transform(combined_sub_queries.loc[combined_sub_queries['subreddit'] == subreddit,'title'])\n",
    "    X_cv_df = pd.DataFrame(X_cv.todense(), columns=cv.get_feature_names())\n",
    "    bar_graph = X_cv_df.sum().sort_values(ascending = False).head(20).plot(kind = 'barh')\n",
    "    plt.title(f'Most Frequent Terms in \"{subreddit}\"', size = 16)\n",
    "    plt.xlabel('Frequency', size = 15)\n",
    "    plt.ylabel('Words', size = 15)\n",
    "    return bar_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 titles from subreddit \"cars\" unsurprisingly contains the word \"car\" many times. We can also derive that the most popular car of discussion is Honda Civic, followed by Ford Mustang, Toyota Camry, Land Rover Range Rover, and Toyota Corolla. We can also see more interest in new cars as opposed to used cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequent_terms('cars', (2,2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular term in the \"teslamotors\" subreddit, \"feature request\", refers to over-the-air (OTA) software updates that Tesla sends out over time. These are mostly wish lists, and sometimes do come true when Tesla reviews feedback. Owners will frequently tweet Elon Musk, and he will respond! The 3rd most populate bigram is \"smart summon\" which refers to the feature of using phones to control the car to drive forward, backward, or even navigate to where you are in a parking lot. \n",
    "<br>We would expect low occurances of these terms in the 'cars' subreddit since most other auto manufacturers are not as software focused, and are lagging behind in this area. \n",
    "<br>We also see frequent occurances of the term \"daily discussion\" and \"support thread\" which we will need to take out since that is a general reddit term and not subreddit specific. \n",
    "<br>Given the high level of Tesla specific terms, we are confident in our ability to accuratly match titles to their respective subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequent_terms('teslamotors', (2,2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "car           564\n",
       "tesla         392\n",
       "model         300\n",
       "new           143\n",
       "cars          136\n",
       "help          100\n",
       "does          100\n",
       "question       94\n",
       "need           78\n",
       "vs             70\n",
       "just           69\n",
       "used           67\n",
       "buy            60\n",
       "know           60\n",
       "request        57\n",
       "feature        57\n",
       "cybertruck     56\n",
       "driving        56\n",
       "looking        54\n",
       "advice         54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data up into X and y.\n",
    "X = combined_sub_queries['title']\n",
    "y = combined_sub_queries['subreddit'].map(lambda x: 1 if x == 'teslamotors' else 0)\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "cv.fit(X)\n",
    "X_cv = cv.transform(X)\n",
    "words = pd.DataFrame(X_cv.todense(), columns=cv.get_feature_names())\n",
    "\n",
    "# Let's look at the most frequently used words.\n",
    "words.sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00         3\n",
       "000       13\n",
       "000rpm     1\n",
       "01         2\n",
       "02         6\n",
       "          ..\n",
       "zero       2\n",
       "zipcar     1\n",
       "zonda      1\n",
       "zones      1\n",
       "zx4        1\n",
       "Length: 4605, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the number of words\n",
    "words.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the character lengths of the words.\n",
    "lengths_of_words = [len(each) for each in words.columns]\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(lengths_of_words, ax=ax, kde = False)\n",
    "plt.title('Histogram of Word Lengths', size = 20)\n",
    "plt.axvline(np.mean(lengths_of_words), 0,350, color = 'red')\n",
    "plt.axvline(np.median(lengths_of_words), 0,350, color = 'green')\n",
    "plt.ylabel('Frequency', size = 15)\n",
    "plt.xlabel('Characters', size = 15)\n",
    "ax.set_xlim(1,20)\n",
    "ax.set_xticks(range(1,20))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the character lengths of the titles.\n",
    "lengths_of_posts = [len(each) for each in X]\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(lengths_of_posts, kde = False, bins = 30) #kde = False\n",
    "plt.title('Histogram of Title Lengths', size = 20)\n",
    "plt.axvline(np.mean(lengths_of_posts), 0,350, color = 'red')\n",
    "plt.axvline(np.median(lengths_of_posts), 0,350, color = 'green')\n",
    "plt.ylabel('Frequency', size = 15)\n",
    "plt.xlabel('Characters', size = 15)\n",
    "ax.set_xlim(1,300)\n",
    "ax.set_xticks(range(0, 300, 20))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.13369528333899"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifying the mean character lenght of titles\n",
    "np.mean(lengths_of_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifying the median character lenght of titles\n",
    "np.median(lengths_of_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cars           1634\n",
       "teslamotors    1313\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the quantities of each class to confirm they are balanced\n",
    "combined_sub_queries['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FSD wording changed to \"coming later this year\"</td>\n",
       "      <td>Looks like as the clock rolls over to 2020 tha...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577865335</td>\n",
       "      <td>mahkus11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature Request: battery preconditioning drivi...</td>\n",
       "      <td>Here in Europe we have many other fast chargin...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577867595</td>\n",
       "      <td>sharpfoam</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My favorite Tesla hack to date!</td>\n",
       "      <td>https://imgur.com/gallery/1kgf4WF\\n\\nUsing a f...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577867764</td>\n",
       "      <td>godloki</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s Q1, where is my model Y?</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577868924</td>\n",
       "      <td>code_name_duchess_18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anyone concerned about model Y Pricing</td>\n",
       "      <td>For those that purchased the first Model 3s, t...</td>\n",
       "      <td>teslamotors</td>\n",
       "      <td>1577868984</td>\n",
       "      <td>I_Shit_Gold_Bars</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Price for a matte paint job or wrap?</td>\n",
       "      <td>I don’t know much about cars but I was wonderi...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567668656</td>\n",
       "      <td>basedwatts250</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Don't you hate when you see a car from one bra...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567671007</td>\n",
       "      <td>young_legendary</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Why do we use larger rear tires for hard accel...</td>\n",
       "      <td>Looking at cars and motorcycles(this is actual...</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567674235</td>\n",
       "      <td>LMGDiVa</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Shillong Tour Packages</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567677088</td>\n",
       "      <td>travenjo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>I want to purchase a car used for family, the ...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>cars</td>\n",
       "      <td>1567677421</td>\n",
       "      <td>cocola-full2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-09-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2947 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0      FSD wording changed to \"coming later this year\"   \n",
       "2    Feature Request: battery preconditioning drivi...   \n",
       "3                      My favorite Tesla hack to date!   \n",
       "4                        It’s Q1, where is my model Y?   \n",
       "5               Anyone concerned about model Y Pricing   \n",
       "..                                                 ...   \n",
       "493               Price for a matte paint job or wrap?   \n",
       "494  Don't you hate when you see a car from one bra...   \n",
       "495  Why do we use larger rear tires for hard accel...   \n",
       "498                             Shillong Tour Packages   \n",
       "499  I want to purchase a car used for family, the ...   \n",
       "\n",
       "                                              selftext    subreddit  \\\n",
       "0    Looks like as the clock rolls over to 2020 tha...  teslamotors   \n",
       "2    Here in Europe we have many other fast chargin...  teslamotors   \n",
       "3    https://imgur.com/gallery/1kgf4WF\\n\\nUsing a f...  teslamotors   \n",
       "4                                            [removed]  teslamotors   \n",
       "5    For those that purchased the first Model 3s, t...  teslamotors   \n",
       "..                                                 ...          ...   \n",
       "493  I don’t know much about cars but I was wonderi...         cars   \n",
       "494                                          [removed]         cars   \n",
       "495  Looking at cars and motorcycles(this is actual...         cars   \n",
       "498                                          [removed]         cars   \n",
       "499                                          [removed]         cars   \n",
       "\n",
       "     created_utc                author  num_comments  score  is_self  \\\n",
       "0     1577865335              mahkus11             3      1     True   \n",
       "2     1577867595             sharpfoam            62      1     True   \n",
       "3     1577867764               godloki            67      1     True   \n",
       "4     1577868924  code_name_duchess_18             0      1     True   \n",
       "5     1577868984      I_Shit_Gold_Bars             0      1     True   \n",
       "..           ...                   ...           ...    ...      ...   \n",
       "493   1567668656         basedwatts250             7      1     True   \n",
       "494   1567671007       young_legendary             2      1     True   \n",
       "495   1567674235               LMGDiVa             5      0     True   \n",
       "498   1567677088              travenjo             0      1     True   \n",
       "499   1567677421          cocola-full2             2      1     True   \n",
       "\n",
       "      timestamp  \n",
       "0    2020-01-01  \n",
       "2    2020-01-01  \n",
       "3    2020-01-01  \n",
       "4    2020-01-01  \n",
       "5    2020-01-01  \n",
       "..          ...  \n",
       "493  2019-09-05  \n",
       "494  2019-09-05  \n",
       "495  2019-09-05  \n",
       "498  2019-09-05  \n",
       "499  2019-09-05  \n",
       "\n",
       "[2947 rows x 9 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sub_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing any potential duplicates\n",
    "combined_sub_queries = combined_sub_queries.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cars           1634\n",
       "teslamotors    1313\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing how balanced the data set is between the two classes\n",
    "combined_sub_queries['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have setup stop words to remove the 'daily discussion' and 'support thread' in our model preparation section, which will run during our modeling section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Preparation (Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up our X and y variables\n",
    "X = combined_sub_queries['title']\n",
    "y = combined_sub_queries['subreddit']\n",
    "\n",
    "# train test splitting our data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up our classes to use later in our pipelines\n",
    "# following code found on https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "    \n",
    "class StemTokenizer:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc)]\n",
    "\n",
    "# https://stackoverflow.com/questions/28384680/scikit-learns-pipeline-a-sparse-matrix-was-passed-but-dense-data-is-required/51127115#51127115\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up custom stop words\n",
    "sw = ['daily discussion', 'support thread']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline is 55.45%. This represents the majority class of subreddit 'cars' of our binary classification problem. We need to outperform this majority baseline score before we can claim usefulness from our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5544621649134713"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = y.value_counts(normalize = True).max()\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Count Vectorized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer is a transformer that tokenizes text by lowercasing and removing punctuation from words. It does not, however, convert the words to their root forms. We have introduced Porter Stemmer and WordNet Lemmatizer as a hyper-parameter in all of our models. Stemming may result in words that are not actual English words, but lemmatizing will root to actual English words. \n",
    "Logistic Regression is a linear where predictions are transformed using logistic function. There is a default penalty of L2, meaning this will automatically use the Ridge penalty and lower coefficients instead of removing them completely. We have included hyper parameters 'lbfgs', 'liblinear', and 'saga' and will have the model determine the best to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cv',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=4000, min_df=1,\n",
       "                                 ngram_range=(1, 8), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<__main__.StemTokenizer object at 0x0000014810B1DB48>,\n",
       "                                 vocabulary=None)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate pipeline.  \n",
    "pipe_cv = Pipeline([\n",
    "    ('cv', CountVectorizer(stop_words = sw)), \n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define grid of parameters to GridSearch over.\n",
    "params_grid = {\n",
    "    'cv__tokenizer': [LemmaTokenizer(), StemTokenizer()],\n",
    "    'cv__max_features': [3950, 4000, 4050],\n",
    "    'cv__stop_words': ['english'],\n",
    "    'cv__ngram_range': [(1,7),(1,8)],\n",
    "    'lr__solver': ['lbfgs', 'liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# GridSearch over pipeline with given grid of parameters.\n",
    "gs_cv = GridSearchCV(pipe_cv, params_grid, cv=5, scoring = 'accuracy')\n",
    "\n",
    "# Fit model.\n",
    "gs_cv.fit(X_train, y_train)\n",
    "\n",
    "gs_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv__max_features': 4000,\n",
       " 'cv__ngram_range': (1, 8),\n",
       " 'cv__stop_words': 'english',\n",
       " 'cv__tokenizer': <__main__.StemTokenizer at 0x14810b1db48>,\n",
       " 'lr__solver': 'liblinear'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8752"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking accuracy score\n",
    "gs_cv_lr_accuracy = np.round(gs_cv.score(X_test, y_test), 4)\n",
    "gs_cv_lr_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 TFIDF Vectorized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency inverse document frequency (TF-IDF) Vectorizer is another tranformer that offers a more sophisticated approach as it builds on Count Vectorizer by increasing weights on words that are frequent in a document or row, but offsets this by decreasing weights on words that are frequent in the corpus, or the number of documents that contain the words in question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvec',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=4150,\n",
       "                                 min_df=1, ngram_range=(1, 5), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_p...,\n",
       "                                 tokenizer=<__main__.StemTokenizer object at 0x0000014814DF8608>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='saga', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Pipeline - TFIDF\n",
    "pipe_tfidf = Pipeline(steps = [('tfidfvec', TfidfVectorizer(stop_words = sw)),     # first tuple is for first step: vectorizer\n",
    "                         ('lr', LogisticRegression())        # second tuple is for second step: model\n",
    "                        ])    \n",
    "\n",
    "# Construct Grid Parameters\n",
    "hyperparams_grid = {\n",
    "                'tfidfvec__tokenizer': [LemmaTokenizer(), StemTokenizer()],\n",
    "                'tfidfvec__max_features': [4050, 4100, 4150],\n",
    "                'tfidfvec__ngram_range': [(1,4),(1,5),(1,6)],\n",
    "                'tfidfvec__stop_words': ['english'],\n",
    "                'lr__solver': ['liblinear', 'lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "gs_tfidf = GridSearchCV(pipe_tfidf, # pipeline object replaces what we usually had as empty model class\n",
    "                 param_grid=hyperparams_grid,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy')\n",
    "\n",
    "gs_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Get best estimator\n",
    "gs_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8657"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking accuracy score\n",
    "gs_tfidf_lr_accuracy = np.round(gs_tfidf.score(X_test, y_test),4)\n",
    "gs_tfidf_lr_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutlinomial Naive Bayes works when we have discrete data, building on Count Vectorizer using the counts of each word or token to classify and predict our y value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cvec',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=5000, min_df=1,\n",
       "                                 ngram_range=(1, 6), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<__main__.LemmaTokenizer object at 0x0000014810B1AB08>,\n",
       "                                 vocabulary=None)),\n",
       "                ('to_dense',\n",
       "                 <__main__.DenseTransformer object at 0x0000014815301C88>),\n",
       "                ('mnb',\n",
       "                 MultinomialNB(alpha=1, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup pipeline for Multinomial Naive Bayes\n",
    "pipe_mnb = Pipeline([\n",
    "     ('cvec', CountVectorizer(stop_words = sw)), \n",
    "     ('to_dense', DenseTransformer()), \n",
    "     ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Construct Grid Parameters\n",
    "hyperparams_grid = {\n",
    "                'cvec__tokenizer': [LemmaTokenizer(), StemTokenizer()],\n",
    "                'cvec__max_features': [5000, 5500],\n",
    "                'cvec__ngram_range': [(1,6),(1,7)],\n",
    "                'cvec__stop_words': ['english'],\n",
    "                'mnb__alpha': [0, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "gs_mnb = GridSearchCV(pipe_mnb, # pipeline object replaces what we usually had as empty model class\n",
    "                 param_grid=hyperparams_grid,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy')\n",
    "\n",
    "gs_mnb.fit(X_train, y_train)\n",
    "\n",
    "# Get best estimator\n",
    "gs_mnb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': 5000,\n",
       " 'cvec__ngram_range': (1, 6),\n",
       " 'cvec__stop_words': 'english',\n",
       " 'cvec__tokenizer': <__main__.LemmaTokenizer at 0x14810b1ab08>,\n",
       " 'mnb__alpha': 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.867"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking accuracy score\n",
    "gs_mnb_accuracy = np.round(gs_mnb.score(X_test, y_test), 4)\n",
    "gs_mnb_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Gaussian Naive Bayes Classifier####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes is used in cases where our features are continuous, hence we will combine this with our TF-IDF Vectorizer. A Gaussian distributions is also called a normal distribtuion, providing a bell shaped curve symmetric about the mean of the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16463\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidfvec',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=5000,\n",
       "                                 min_df=1, ngram_range=(1, 8), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<__main__.LemmaTokenizer object at 0x00000148159DFB48>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('to_dense',\n",
       "                 <__main__.DenseTransformer object at 0x0000014814E88088>),\n",
       "                ('gnb', GaussianNB(priors=None, var_smoothing=0.9))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup pipeline for Multinomial Naive Bayes\n",
    "pipe_gnb = Pipeline([\n",
    "     ('tfidfvec', TfidfVectorizer(stop_words = sw)), \n",
    "     ('to_dense', DenseTransformer()), \n",
    "     ('gnb', GaussianNB())\n",
    "])\n",
    "\n",
    "# Construct Grid Parameters\n",
    "hyperparams_grid = {\n",
    "                'tfidfvec__tokenizer': [LemmaTokenizer(), StemTokenizer()],\n",
    "                'tfidfvec__max_features': [5000, 5500, 6000],\n",
    "                'tfidfvec__ngram_range': [(1,7), (1,8)],\n",
    "                'tfidfvec__stop_words': ['english'],\n",
    "                'gnb__var_smoothing': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "gs_gnb = GridSearchCV(pipe_gnb, # pipeline object replaces what we usually had as empty model class\n",
    "                 param_grid=hyperparams_grid,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy')\n",
    "\n",
    "gs_gnb.fit(X_train, y_train)\n",
    "\n",
    "# Get best estimator\n",
    "gs_gnb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gnb__var_smoothing': 0.9,\n",
       " 'tfidfvec__max_features': 5000,\n",
       " 'tfidfvec__ngram_range': (1, 8),\n",
       " 'tfidfvec__stop_words': 'english',\n",
       " 'tfidfvec__tokenizer': <__main__.LemmaTokenizer at 0x148159dfb48>}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_gnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8589"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking accuracy score\n",
    "gs_gnb_accuracy = np.round(gs_gnb.score(X_test, y_test), 4)\n",
    "gs_gnb_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy score is : 0.5545.\n",
      "\n",
      "Training accuracy score for our Count Vectorized Logistic Regression model is: 0.9624.\n",
      "Testing accuracy score for our Count Vectorized Logistic Regression model is: 0.8752.\n",
      "\n",
      "Training accuracy score for our TF-IDF Vectorized Logistic Regression model is: 0.9376.\n",
      "Testing accuracy score for our TF-IDF Vectorized Logistic Regression model is: 0.8657.\n",
      "\n",
      "Training accuracy score for our Multinomial Naive Bayes Classifier model is: 0.9439.\n",
      "Testing accuracy score for our Multinomial Naive Bayes Classifier model is: 0.867.\n",
      "\n",
      "Training accuracy score for our Gaussian Naive Bayes model is: 0.9434.\n",
      "Testing accuracy score for our Gaussian Naive Bayes model is: 0.8589.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate predictions\n",
    "print(f'Baseline accuracy score is : {np.round(baseline, 4)}.')\n",
    "print()\n",
    "print(f'Training accuracy score for our Count Vectorized Logistic Regression model is: {round(gs_cv.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Count Vectorized Logistic Regression model is: {gs_cv_lr_accuracy}.')\n",
    "print()\n",
    "print(f'Training accuracy score for our TF-IDF Vectorized Logistic Regression model is: {round(gs_tfidf.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our TF-IDF Vectorized Logistic Regression model is: {gs_tfidf_lr_accuracy}.')\n",
    "print()\n",
    "print(f'Training accuracy score for our Multinomial Naive Bayes Classifier model is: {round(gs_mnb.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Multinomial Naive Bayes Classifier model is: {gs_mnb_accuracy}.')\n",
    "print()\n",
    "print(f'Training accuracy score for our Gaussian Naive Bayes model is: {round(gs_gnb.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Gaussian Naive Bayes model is: {gs_gnb_accuracy}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize list of lists \n",
    "scores_list = [['CountVectorized Logistic', gs_cv_lr_accuracy], ['TFIDFVectorized Logistic', gs_tfidf_lr_accuracy], \n",
    "        ['Multinomial Naive Bayes', gs_mnb_accuracy], ['Gaussian Naive Bayes', gs_gnb_accuracy]]\n",
    "  \n",
    "# Create the pandas DataFrame \n",
    "scores_df = pd.DataFrame(scores_list, columns = ['Classifier', 'Test Accuracy Score']).sort_values('Test Accuracy Score')\n",
    "# scores_df.sort_values(scores_df, ascending = False)\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "ax = sns.barplot(x=\"Test Accuracy Score\", y='Classifier', data=scores_df, palette = 'Blues_d')\n",
    "plt.title(f'Test Accuracy Scores for All Models', size = 20)\n",
    "plt.xlabel('Test Accuracy Score', size = 18)\n",
    "plt.xticks(ticks = [0, 0.2, 0.4, 0.6, 0.8, 1], size = 15)\n",
    "plt.yticks(size = 15)\n",
    "plt.ylabel('Classifier', size = 18)\n",
    "\n",
    "for i in ax.patches:\n",
    "    # get_width pulls left or right; get_y pushes up or down\n",
    "    ax.text(i.get_width(), i.get_y()+.31, \\\n",
    "            str(round((i.get_width()), 3)), fontsize=15, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model appears to be the Count Vectorized Logistic Regression Model with an testing accuracy score of 87.52%, so we will select this to evaluate. This is 32 points above our baseline of 55.45 %, or nearly 58% increase. We ran multiple iterations of our models and found that as titles of posts changed over time, our winning model would switch between TFIDF Vectorized Logistic regression and Multinomial Naive Bayes, but we never had an instance of the Gaussian Naive Bayes scoring highest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are getting our best model, the 2nd value of our grid search from Count Vectorized Logistic Regression model, and our best transformer, the 1st value. This will allow us to call dot(.) attributes and methods to eventually pull our coefficients, which will help us identify the most useful words for our model to predict the subreddit 'teslamotors'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs_cv.best_estimator_[1]\n",
    "best_transformer = gs_cv.best_estimator_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = best_transformer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = best_model.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    'feature':features,\n",
    "    'coef': np.round(coefs, 2),\n",
    "    'exp_coef': np.round(np.exp(coefs), 2)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exp_coef below represents the odds that a title is classified as under subreddit 'teslamotors' given the word in the feature column exists in the document. It is not surprising that given a document contains the word \"tesla\" it is 76.87 times as likely to be classified under 'teslamotors'. Terms beyond the obvious make and model include autopilot: a driver assistance safety feature with automatic emergency braking, V10: Tesla's biggest software version update, and summon: functionality to move your car from your phone like a remote control. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef</th>\n",
       "      <th>exp_coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3417</th>\n",
       "      <td>tesla</td>\n",
       "      <td>4.34</td>\n",
       "      <td>76.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>cybertruck</td>\n",
       "      <td>2.88</td>\n",
       "      <td>17.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>autopilot</td>\n",
       "      <td>2.17</td>\n",
       "      <td>8.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767</th>\n",
       "      <td>v10</td>\n",
       "      <td>1.90</td>\n",
       "      <td>6.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>model</td>\n",
       "      <td>1.87</td>\n",
       "      <td>6.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>m3</td>\n",
       "      <td>1.83</td>\n",
       "      <td>6.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3322</th>\n",
       "      <td>summon</td>\n",
       "      <td>1.82</td>\n",
       "      <td>6.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>mode</td>\n",
       "      <td>1.65</td>\n",
       "      <td>5.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>elon</td>\n",
       "      <td>1.63</td>\n",
       "      <td>5.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>deliveri</td>\n",
       "      <td>1.59</td>\n",
       "      <td>4.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>app</td>\n",
       "      <td>1.42</td>\n",
       "      <td>4.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>fsd</td>\n",
       "      <td>1.41</td>\n",
       "      <td>4.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3720</th>\n",
       "      <td>updat</td>\n",
       "      <td>1.37</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>charg</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3</td>\n",
       "      <td>1.18</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>saw</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3334</th>\n",
       "      <td>supercharg</td>\n",
       "      <td>1.14</td>\n",
       "      <td>3.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>usb</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>spotifi</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>frunk</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature  coef  exp_coef\n",
       "3417       tesla  4.34     76.87\n",
       "1354  cybertruck  2.88     17.84\n",
       "879    autopilot  2.17      8.79\n",
       "3767         v10  1.90      6.69\n",
       "2380       model  1.87      6.52\n",
       "2243          m3  1.83      6.24\n",
       "3322      summon  1.82      6.17\n",
       "2376        mode  1.65      5.19\n",
       "1616        elon  1.63      5.11\n",
       "1429    deliveri  1.59      4.93\n",
       "824          app  1.42      4.16\n",
       "1838         fsd  1.41      4.11\n",
       "3720       updat  1.37      3.93\n",
       "1161       charg  1.35      3.87\n",
       "413            3  1.18      3.26\n",
       "3098         saw  1.15      3.17\n",
       "3334  supercharg  1.14      3.13\n",
       "3734         usb  1.11      3.03\n",
       "3244     spotifi  1.05      2.86\n",
       "1837       frunk  1.04      2.82"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values(by = 'coef', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have mapped a confusion matrix of the actual subreddit vs our predictions. We were better at accurately predicting titles that belonged in the cars subreddit than teslamotors. We were 97.3 % accurate with our predictions from actual subreddit 'cars', but only 75.3% accurate with our predictions from actual subreddit 'teslamotors'. This means there are still normal car terms used in subreddit 'teslamotors' that our model was unable to differentiate between the two, but this is expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confm = confusion_matrix(y_test, y_pred)\n",
    "columns = ['cars', 'teslamotors'] # need to fix this\n",
    "df_cm = pd.DataFrame(confm, index=columns, columns=columns)\n",
    "# sns.heatmap(df_cm, cmap='Oranges', annot=True)\n",
    "plt.figure(figsize = (5, 4))\n",
    "plt.title('Confusion Matrix', size = 22)\n",
    "sns.heatmap(df_cm, annot=True, fmt = 'd', annot_kws={\"size\": 15}, cmap = 'RdBu')\n",
    "plt.ylabel('Actual', size = 20)\n",
    "plt.xlabel('Predictions', size = 20)\n",
    "plt.yticks(rotation = 0, size = 15)\n",
    "plt.xticks(size = 15)\n",
    "\n",
    "# shoutout to SalMac86 on https://github.com/mwaskom/seaborn/issues/1773\n",
    "# fix for mpl bug that cuts off top/bottom of seaborn viz\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "plt.show()\n",
    "\n",
    "## add xlabel and ylabel for predicted vs actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8752\n",
      "Recall/Sensitivity: 0.753\n",
      "Specificity: 0.9731\n",
      "Precision: 0.9574\n"
     ]
    }
   ],
   "source": [
    "# additional metrics\n",
    "print('Accuracy:',  round((tp + tn) / (tp+fp+tn+fn), 4))\n",
    "print('Recall/Sensitivity:', round((tp) / (tp+fn), 4))\n",
    "print('Specificity:', round((tn) / (tn+fp), 4))\n",
    "print('Precision:', round((tp) / (tp+fp), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our models were more successful than the baseline in classifying the subreddit, but our Count Vectorized Logistic Model outperformed the rest with an accuracy score of 87.5%. We are successful in building a model that can reasonably classify a title of a reddit post to belonging to subreddit 'teslamotors' or 'cars'. We were also successfull in identifying key words beyond the make and model that hold significance, such as autopilot, v10, and summon. Tesla spends $0 on advertising, so it is not surprising if the population is not familiar with these features in relation to vehicles. If we can educate the public on new technologies that exist on Tesla vehicles, specifically context behind the highest non-make and model coefficients of our model from subreddit 'teslamotors', we can potentially reduce the dependency on gas, slow down global warming and pollution, and save some lives with autopilot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
